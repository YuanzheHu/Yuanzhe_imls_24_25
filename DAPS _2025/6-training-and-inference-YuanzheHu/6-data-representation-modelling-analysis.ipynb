{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b86-JY0HJBcZ"
   },
   "source": [
    "<div width=50% style=\"display: block; margin: auto\">\n",
    "    <img src=\"figures/ucl-logo.svg\" width=100%>\n",
    "</div>\n",
    "\n",
    "\n",
    "### [UCL-ELEC0136 Data Acquisition and Processing Systems 2024]()\n",
    "University College London\n",
    "\n",
    "# **Lab 7: Data Representation, Modelling & Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIEGzXEwYqcz"
   },
   "source": [
    "In the first part of this lab, we will use weather data extracted from Meteonorm for five different cities in the world. In the second part of the lab, we will use the London Santander Cycle data as time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Rii4Y-4cljH"
   },
   "source": [
    "### Objectives\n",
    "* Learn how to understand and cluster linear data with **linear dimensionality reduction** techniques, such as PCA.\n",
    "* Learn how to analyse non-linear data through **non-linear dimensionality reduction** methods, such as Kernel-PCA.\n",
    "* Learn to **plot** and represent data and results.\n",
    "* Learn how to analyse data using different types of plots.\n",
    "* Learn forecasting data using **Facebook Prophet**.\n",
    "* Learn time-series **prediction** and **forecasting**.\n",
    "\n",
    "### Outline\n",
    "This notebook has 3 main parts:\n",
    "\n",
    "0. [Setup](#0---setup)\n",
    "1. [Linear Dimensionality Reduction](#1---linear-dimensionality-reduction-----principal-component-analysis-pca).\n",
    "2. [Non-Linear Dimensionality Reduction](#2---non-linear-dimensionality-reduction).\n",
    "3. [Time Series Forecasting](#3---time-series-forecasting).\n",
    "\n",
    "\n",
    "<hr width=70% style=\"float: left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 - Setup\n",
    "The first thing we need to do is to install the required packages for this lab. The packages we are using are:\n",
    "- `numpy`: numerical computing library\n",
    "- `pandas`: data manipulation and analysis library with data structures for efficient data handling and manipulation\n",
    "- `matplotlib`: plotting library that is suitable for various types of visualizations\n",
    "- `scikit-learn`: simple and efficient library of tools for data mining and data analysis\n",
    "- `seaborn`: data visualization library that provides a high-level interface for creating statistical graphics\n",
    "- `openpyxl`: library for reading and writing Excel files\n",
    "- `prophet`: forecasting library that offers a tool for time series data, and utilizes an additive model to predict trends and seasonality in data with daily observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"background-color: #FFCDD2; border: 1px solid red; padding: 10px;\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Task 0.1: Install the required packages </b>\n",
    "\n",
    "Add the packages to your requirements file and install all of them.\n",
    "\n",
    "<summary>üîé Hint</summary>\n",
    "\n",
    "Note that:\n",
    "- you can add fbprophet to your `requirements.txt` file as `prophet` or `prophet`.\n",
    "- if the pip command does not work, try to install it using the conda command: `conda install -c conda-forge prophet`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SkIo9UnYN5br"
   },
   "source": [
    "# 1 - Linear Dimensionality Reduction ---  Principal Component Analysis (PCA)\n",
    "\n",
    "Data is one of the main drivers behind most of Machine Learning applications. Analysing the data prior to any usage is of crucial importance since data can be complicated and sometimes may be challenging to understand the meaning of all its components such as data points, attributes, characteristics, variables, and values. These data components collectively represent information within the set of data and it is often complicated to grasp which parts carry information.\n",
    "\n",
    "Dimensionality reduction is a technique that helps us gain a better macro-level understanding of the data by reducing the number of features that describe a dataset such that only the most information-rich parts remain.\n",
    "\n",
    "Principal Component Analysis (PCA) is a simple yet powerful technique used for dimensionality reduction. Through it, we can directly decrease the number of feature variables, thereby narrowing down the important features and saving on computations. From a high-level view, PCA has three main steps:\n",
    "\n",
    "1. Compute the **covariance matrix** of the data.\n",
    "2. Compute the **eigen values** and **vectors** of this covariance matrix.\n",
    "3. Use the eigenvalues and vectors to select only the most important feature dimensions that carry enough information to approximate the original data.\n",
    "\n",
    "\n",
    "**PCA effectively involves computing the eigenvalues and eigenvectors of the input data, and then using these as a new co-ordinate basis.** \n",
    "\n",
    "\n",
    "<div width=50% style=\"display: block; margin: auto\">\n",
    "    <img src=\"figures/pca.png\" width=50%>\n",
    "</div>\n",
    "\n",
    "By only keeping the eigenvectors corresponding to the largest eigenvalues, we end up with a reduced set of input coordinates that represent better data features.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<h4>üë©‚Äçüíª [Optional]</h4>\n",
    "\n",
    "- Short read: [PCA Explianed Visually](https://setosa.io/ev/principal-component-analysis/), which is the source of the above figure.\n",
    "- Longer read: [PCA with numpy tutorial](https://plot.ly/python/v3/ipython-notebooks/principal-component-analysis/).\n",
    "\n",
    "</div>\n",
    "\n",
    "We will use weather data downloaded using [Meteonorm](https://meteonorm.com/en/download) to illustrate the entire process in the script below.\n",
    "Meteonorm is a software that collects accurate weather data for representative years and historical time series for any place on Earth. You also have the flexibility to choose from more than 30 different weather parameters to include in your dataset. For this session, we will be using the DEMO version which allows us to download data for only 5 cities in 2005: Bern, Johannesburg, San Francisco, Perth, and Brasilia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PminAxfOIR5"
   },
   "source": [
    "\n",
    "\n",
    "The data is stored in comma-separated (.csv) files called *city_name*-hour.dat.\n",
    "In this analysis, we have selected the following 7 main weather parameters:\n",
    "- Global radiation ($W/m^2$)\n",
    "- Diffuse radiation ($W/m^2$)\n",
    "- Temperature ($¬∞C$)\n",
    "- Wind speed ($m/s$)\n",
    "- Relative humidity ($\\%$)\n",
    "- Cloud cover ($oktas$)\n",
    "- Precipitation ($mm$)\n",
    "\n",
    "**NOTE:** In meteorology, an okta is a unit of measurement used to describe the amount of cloud cover.\n",
    "SKC = Sky clear (0 oktas); FEW = Few (1 to 2 oktas); SCT = Scattered (3 to 4 oktas); BKN = Broken (5 to 7 oktas); OVC = Overcast (8 oktas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mc1OOwr_JKyA"
   },
   "outputs": [],
   "source": [
    "#@title Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import the helper functions\n",
    "from helper_functions import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xe6JyVbGMefe"
   },
   "outputs": [],
   "source": [
    "#@title Load data\n",
    "df1 = pd.read_table('./lab7-data/weather-data/Bern-hour.dat', sep=',', header=None)\n",
    "df2 = pd.read_table('./lab7-data/weather-data/Johannesburg-hour.dat', sep=',', header=None)\n",
    "df3 = pd.read_table('./lab7-data/weather-data/SanFrancisco-hour.dat', sep=',', header=None)\n",
    "df4 = pd.read_table('./lab7-data/weather-data/Perth-hour.dat', sep=',', header=None)\n",
    "df5 = pd.read_table('./lab7-data/weather-data/Brasilia-hour.dat', sep=',', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "noVG14MXNxAY"
   },
   "source": [
    "`create_dataframe()` is a function that assigns names to the imported dataset's columns and creates a DateTimeIndex for better manipulation of the time-series data. The function also creates an additional **class** column to distinguish each city for the PCA analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZohEn7eOJ7v"
   },
   "outputs": [],
   "source": [
    "#@title Preprocess data frames\n",
    "df_bern = create_dataframe(df1, cls=0)\n",
    "df_perth = create_dataframe(df4, cls=1)\n",
    "df_johannesburg = create_dataframe(df2, cls=2)\n",
    "df_sanfrancisco = create_dataframe(df3, cls=3)\n",
    "df_brasilia = create_dataframe(df5, cls=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkAoUFGqPD1q"
   },
   "source": [
    "## Applying PCA to two cities\n",
    "\n",
    "As an example, we have decided to apply the PCA to the cities of Perth and Bern, since they belong to the Southern and Northern hemispheres, respectively. Due to the axial tilt of Earth relative to the Sun and the ecliptic plane, the seasons are inverted between the two hemispheres.\n",
    "\n",
    "Therefore, we expect to find clear differences in weather data by extracting only the four months in which it is winter in Bern and summer in Perth, i.e. January, February, November, and December. Via PCA, we can reduce the dimensionality of the dataset by projecting the whole data **into the few necessary dimensions to discriminate between the two hemispheres**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rnPrTHqOP3WN"
   },
   "outputs": [],
   "source": [
    "# we can save the desired period by dropping the months in the middle (i.e. March to October)\n",
    "df_perth_summer = df_perth.drop(df_perth.loc['2005-03-01 00:00:00':'2005-10-31 23:00:00', :].index, axis=0)\n",
    "\n",
    "# print the dataframe to see the table\n",
    "df_perth_summer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FElzYJkzQSSi"
   },
   "outputs": [],
   "source": [
    "# we can save the desired period, by dropping the months in the middle (i.e. March to October)\n",
    "df_bern_winter = df_bern.drop(df_bern.loc['2005-03-01 00:00:00':'2005-10-31 23:00:00', :].index, axis=0)\n",
    "\n",
    "# print the dataframe to see the table\n",
    "df_bern_winter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dataframe called df_pca on which we will performed the PCA algorithm. **At this point, df_pca still contains normal data as PCA has not been performed yet.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E2ivXIgnQj6b"
   },
   "outputs": [],
   "source": [
    "# we now combine (concatenate) the data of both cities to use for our PCA analysis\n",
    "df_pca = pd.concat([df_bern_winter, df_perth_summer], keys=['Bern','Perth'], axis=0, join='inner')\n",
    "\n",
    "# print the dataframe to see the table\n",
    "df_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Aj1UkU8Qym7"
   },
   "source": [
    "Here, we define the inputs (X) to the PCA algorithm to be all the weather parameters, and the target (y) is the city identifier, *class*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nu2KiakWQzbL"
   },
   "outputs": [],
   "source": [
    "# extract the values of the features / attributes of the data from the DataFrame 'df_pca'\n",
    "X = df_pca.loc[:, 'global radiation':'precipitation'].values\n",
    "\n",
    "# extract the values of the target variable ('class') from the DataFrame 'df_pca'\n",
    "y = df_pca.loc[:, 'class'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lPEH9VN5RX0A"
   },
   "source": [
    "PCA returns a sub-space that maximizes the variance along the feature vectors. Therefore, to properly measure the variance of those feature vectors, the distribution of values on each feature must be of equal variance (otherwise the PCA algorithm would be biased towards features with large variance and range). \n",
    "\n",
    "To do this, we first normalise our data to have zero mean and unit-variance such that each feature will be weighted equally in our calculations.\n",
    "\n",
    "This is done by using the [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) module in **scikit-learn**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "438Vwg48Rg1S"
   },
   "outputs": [],
   "source": [
    "# import StandardScaler from the scikit-learn package\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scale the input features of the data\n",
    "X_std = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpoFXpYtRtqK"
   },
   "source": [
    "scikit-learn implements a module for computing the PCA: `fit_transform()`. You can specify the desired number of principal components or you can take the ones that hold most of the variance (or information) about the dataset. Depending on the problem and the data, the first 2 or 3 principal components can be sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cGCOPRD_R1ao"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# creating the PCA model\n",
    "pca = PCA()\n",
    "\n",
    "# using the PCA model on our standardized data\n",
    "Y = pca.fit_transform(X_std)\n",
    "\n",
    "# contains the proportion of the total variance in the data explained by each principal component\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3JYGv4SYTNR8"
   },
   "source": [
    "How much of the variance is explained by the first two principal components?\n",
    "\n",
    "Let's see whether or not it is enough to clearly distinguish between Bern and Perth.\n",
    "\n",
    "Here is an example of how the data looks like for the first principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clikbsvUTUy3"
   },
   "outputs": [],
   "source": [
    "# create a new figure and axis for plotting with a specified size\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# create a scatter plot for data points corresponding to 'Bern' (class 0), using 'o' markers and labeling as 'Bern'\n",
    "ax.scatter(Y[:, 0][y == 0], np.zeros((Y[y == 0].shape[0], 1)), marker='o', s=50, label='Bern')\n",
    "\n",
    "# create a scatter plot for data points corresponding to 'Perth' (class 1), using 'x' markers and labeling as 'Perth'\n",
    "ax.scatter(Y[:, 0][y == 1], np.zeros((Y[y == 1].shape[0], 1)), marker='x', s=10, label='Perth')\n",
    "\n",
    "# set the x-axis limits to [-6, 6]\n",
    "ax.set_xlim([-6, 6])\n",
    "\n",
    "# set the y-axis limits to [-1, 1]\n",
    "ax.set_ylim([-1, 1])\n",
    "\n",
    "# set the x-axis label\n",
    "ax.set_xlabel('Principal Component 1')\n",
    "\n",
    "# set the title for the plot\n",
    "ax.set_title('Scatter plot for the cities of Bern and Perth - Jan, Feb, Nov, and Dec')\n",
    "\n",
    "# add a legend to the plot\n",
    "plt.legend()\n",
    "\n",
    "# display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSxS58Oualek"
   },
   "source": [
    "You can see that there is a big overlap between the two classes (cities).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jeG4nhYc8DO"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"background-color: #FFCDD2; border: 1px solid red; padding: 10px;\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Task 1.1: Plot the principal components </b>\n",
    "\n",
    "Plot the 2D scatterplot with the first two principal components.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3cYK4QveakUr"
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task:\n",
    "#   Scatterplot with the first two principal components\n",
    "###########################\n",
    "\n",
    "### TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOAFClU7dPbh"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"background-color: #FFCDD2; border: 1px solid red; padding: 10px;\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Task 1.2: Plot the principal components </b>\n",
    "\n",
    "Plot the 3D scatterplot using the first three principal components.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vZg00VDvc5f4"
   },
   "outputs": [],
   "source": [
    "# how to generate 3d figures using matplotlib\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "### TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: What happens when your data is linearly inseparable / dependent?</b>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Was the data used in the  first part of this session linearly separable?</b>\n",
    "</div>\n",
    "\n",
    "Note that that PCA is a linear projection technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-q9YRsZdMTI"
   },
   "source": [
    "## 2 - Non Linear Dimensionality Reduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otgLlb5Yd72f"
   },
   "source": [
    "### 2.1 Kernel Principal Component Analysis (K-PCA)\n",
    "\n",
    "The first non-linear approach we will look at is Kernel PCA which uses a non-linear kernel function with standard PCA to achieve non-linear dimensionality reduction. In practice, it projects the linearly inseparable data into a higher dimensional space that can be linearly separated using PCA. The main steps of KPCA are:\n",
    "1. Select a non-linear kernel mapping\n",
    "2. Construct the normalized kernel matrix\n",
    "3. Find the eigenvalues and vectors of this matrix\n",
    "4. For each data point, use the eigenvalues and vectors to obtain its principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<h4>üë©‚Äçüíª [Optional]</h4>\n",
    "\n",
    "- Most common kernel functions are Gaussian, Sigmoid and Polynomial. You can learn more details about Kernel PCA [here](http://axon.cs.byu.edu/~martinez/classes/778/Papers/KernelPCA.pdf).\n",
    "- You might also find it useful to read about [Kernel SVM](https://scikit-learn.org/stable/modules/svm.html) since it has many conceptual similarities to  Kernel PCA.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aeoUV53cgwaT"
   },
   "source": [
    "For this part, we will use the same dataset as in the first part but the analysis will be done for different cities. We will apply the Kernel PCA to the cities of Perth and Johannesburg and we are going to use only January, February, November, and December when it is summer in both of these cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6gd1KMgsis95"
   },
   "outputs": [],
   "source": [
    "#@title Load and preprocess data as in PCA but just for 'Perth' and 'Johannesburg'\n",
    "df2 = pd.read_table('./lab7-data/weather-data/Johannesburg-hour.dat', sep=',', header=None)\n",
    "df4 = pd.read_table('./lab7-data/weather-data/Perth-hour.dat', sep=',', header=None)\n",
    "\n",
    "# use the same create_dataframe() function used in task 6.1 to create the dataframes\n",
    "df_perth = create_dataframe(df4, cls=1)\n",
    "df_johannesburg = create_dataframe(df2, cls=2)\n",
    "\n",
    "# identify the desired period by dropping the months in the middle (i.e. March to October)\n",
    "df_perth_summer = df_perth.drop(df_perth.loc['2005-03-01 00:00:00':'2005-10-31 23:00:00', :].index, axis=0)\n",
    "df_johannesburg_summer = df_johannesburg.drop(df_johannesburg.loc['2005-03-01 00:00:00':'2005-10-31 23:00:00', :].index, axis=0)\n",
    "\n",
    "# combine (concatente) the data of both cities for our PCA analysis\n",
    "df_pca = pd.concat([df_perth_summer,df_johannesburg_summer], keys=['Perth','Johannesburg'], axis=0, join='inner')\n",
    "\n",
    "# print the resulting dataframe\n",
    "df_pca\n",
    "\n",
    "# define the input parameters (X) and the target vairbale (y)\n",
    "# extract the values of the features ('global radiation' to 'precipitation') from the DataFrame 'df_pca'\n",
    "X = df_pca.loc[:, 'global radiation':'precipitation'].values\n",
    "\n",
    "# extract the values of the target variable ('class') from the DataFrame 'df_pca'\n",
    "y = df_pca.loc[:, 'class'].values\n",
    "\n",
    "# scale the data using StandardScaler already imported in task 6.1\n",
    "# StandardScaler scales the data by ensuring that the transformed data has a mean of 0 and a standard deviation of 1 for each feature\n",
    "X_std = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzHfkSDXje7p"
   },
   "source": [
    "Similar to PCA, scikit-learn provides `fit_transform()` for calculating Kernel PCA. You can specify different arguments such as the number of principal components, the kernel function, and parameters related to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qgENMDZRjkMp"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "# create a KernelPCA (Kernel Principal Component Analysis) model using the Radial Basis Function (RBF) kernel\n",
    "kpca = KernelPCA(kernel='rbf')\n",
    "# fit the model to the standardized data 'X_std' and transform it to 'Y_k' using kernel-based dimensionality reduction\n",
    "Y_k = kpca.fit_transform(X_std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjJbtkCyjpPM"
   },
   "source": [
    "Compute the explained variance and ratio since kernel PCA does not have a built-in function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V11I0VIsjsPS"
   },
   "outputs": [],
   "source": [
    "# calculate the explained variance for each principal component in 'Y_k' by computing the variance along each axis\n",
    "explained_variance = np.var(Y_k, axis=0)\n",
    "\n",
    "# compute the explained variance ratio for each principal component by dividing the explained variance by the total variance\n",
    "explained_variance_ratio = explained_variance / np.sum(explained_variance)\n",
    "\n",
    "# the 'explained_variance_ratio' now contains the proportion of total variance explained by each principal component\n",
    "explained_variance_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBBi58lBdZiY"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"background-color: #FFCDD2; border: 1px solid red; padding: 10px;\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Task 2.1: Separate the data and plot the principal components </b>\n",
    "\n",
    "- Use PCA to separate the data.\n",
    "- Plot the 3D scatterplot with the first three principal components.\n",
    "\n",
    "<summary>üîé Hint</summary>\n",
    "\n",
    "You can use the function `plot_3d_vis()` that we wrote for you to make the plotting of the 3D principal components easier and faster. This function takes as input: \n",
    "- `Y`: the transformed data.\n",
    "- `y`: the target variable.\n",
    "- `label_1`: the label for the first set of data.\n",
    "- `label_2`: the label for the second set of data.\n",
    "- `title`: the title of the 3D scatterplot.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2tO46eBTj4dx"
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task:\n",
    "#   Use PCA for dimensionality reduction\n",
    "###########################\n",
    "\n",
    "\n",
    "### TO DO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWPKq9u-dncz"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"background-color: #FFCDD2; border: 1px solid red; padding: 10px;\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Task 2.2: Different kernel functions </b>\n",
    "\n",
    "Try different kernel functions and **compare** the results by visualizing. Try these functions:\n",
    "- poly.\n",
    "- cosine.\n",
    "- linear.\n",
    "\n",
    "<summary>üîé Hint</summary>\n",
    "\n",
    "You can also use the function `plot_3d_vis()` for plotting the 3D principal components.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "blEttpIlpE2O"
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task:\n",
    "#   Use kernel=poly, cosine, precomputed in KernelPCA\n",
    "###########################\n",
    "\n",
    "### TO DO\n",
    "# using the poly kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rJddyDHCpQFr"
   },
   "outputs": [],
   "source": [
    "### TO DO\n",
    "# using the cosine kernel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4hY1B48qYVg"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"background-color: #FFCDD2; border: 1px solid red; padding: 10px;\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Task 2.3: Try a linear kernel function </b>\n",
    "\n",
    "- What happens when you select a linear kernel? \n",
    "- Can you compare the results to standard PCA?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YMgY30tjqa2J"
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task:\n",
    "#   Use kernel=linear in KernelPCA\n",
    "###########################\n",
    "\n",
    "# using a linear kernel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZqUOzzKO4wO"
   },
   "source": [
    "Comparing the results above for the three different kernel functions (poly, cosine, and linear), we notice that:\n",
    "\n",
    "\n",
    "1.   when plotting the principal components of the 'poly' function, the points are compacted in a small portion of the space.\n",
    "2.   whereas, when plotting the principal components of the 'cosine' function, the points are widely distributed in the space, creating a spherical form with the points of both cities strongly overlapping.\n",
    "3. lastly, when plotting the principal components of the 'linear' function, the points are scattered in the space showing the difference between the two distinct cities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6eTz8AXVufh7"
   },
   "source": [
    "### **[OPTIONAL]** 2.2 t-Distributed Stochastic Neighbor Embedding (t-SNE) (up to the student)\n",
    "\n",
    "t-Distributed Stochastic Neighbor Embedding (t-SNE) is a non-linear approach for dimensionality reduction used for exploring and visualizing high-dimensional data. It uses gradient descent to minimize the Kullback-Leibler divergence between the distribution that measures pairwise similarities of the original high-dimensional inputs and the distribution that measures pairwise similarities of the corresponding embedded in lower-dimension points.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<h4>üë©‚Äçüíª [Optional]</h4>\n",
    "\n",
    "- You can find a summary of the algorithmic steps [here](https://uk.mathworks.com/help/stats/t-sne.html) or more details in the [original paper](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf).\n",
    "- There are quite few hyperparameters to control during optimization process (e.g. perplexity, learning rate, number of iterations etc.). You can find a very informative guide on how to deal with different situations on [How to Use t-SNE Effectively](https://distill.pub/2016/misread-tsne/ ).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6bekeGWu1YR"
   },
   "source": [
    "The `sklearn.manifold` module implements the t-distributed Stochastic Neighbor Embedding. We apply the t-SNE on the same data we defined in task 2.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vYz6igGRu7TK"
   },
   "outputs": [],
   "source": [
    "from sklearn import manifold\n",
    "\n",
    "# create a t-SNE object with specified parameters\n",
    "tsne = manifold.TSNE(n_components=2, init='random',\n",
    "                     random_state=0, perplexity=5)\n",
    "\n",
    "# transform the input data 'X' into a 2D space using t-SNE\n",
    "Y = tsne.fit_transform(X)\n",
    "\n",
    "# print the shape of the transformed data (number of data points, 2 dimensions)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LsFJKR_ieD_M"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"background-color: #FFCDD2; border: 1px solid red; padding: 10px;\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Task 2.4: Visualize </b>\n",
    "\n",
    "Visualize the results of t-SNE.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E_1m-Q77vCAH"
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task:\n",
    "#   3D Scatterplot with the transformed output of t-SNE\n",
    "###########################\n",
    "\n",
    "### TO DO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z2v7fXmgeLDB"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"background-color: #FFCDD2; border: 1px solid red; padding: 10px;\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Task 2.5: Experiment with perplexity </b>\n",
    "\n",
    "Increase the perplexity a lot. What do you notice? What happens when it is higher that the data points? You can also try to change other parameters and compare to see their effect on the outcome.\n",
    "\n",
    "Plot and visualize.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vdmrxcNlvYEb"
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task:\n",
    "#   Change perplexity parameter\n",
    "###########################\n",
    "\n",
    "### TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tO7LIFovvdrb"
   },
   "outputs": [],
   "source": [
    "# visualization\n",
    "\n",
    "### TO DO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5CACUJYwTBxt"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"background-color: #FFCDD2; border: 1px solid red; padding: 10px;\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª [OPTIONAL]: Task 2.6: Compare t-SNE to PCA </b>\n",
    "\n",
    "Plot and visualize the 3D scatterplot.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vYG1m_hOTPHE"
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task:\n",
    "#   Use PCA for dimensionality reduction\n",
    "###########################\n",
    "\n",
    "\n",
    "### TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LsE1hoFRTPme"
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task:\n",
    "#   3D Scatterplot of the t-SNE and PCA results\n",
    "###########################\n",
    "\n",
    "\n",
    "### TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Q: Compare t-SNE results to PCA. What are the advantages and disadvantages of each approach? What happens if you use PCA before t-SNE?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLGuJoX8_60V"
   },
   "source": [
    "## 3 - Time Series Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M4LAsic2AMKY"
   },
   "source": [
    "This lab session addresses the common problems of modeling time series data to make predictions on their future behavior.\n",
    "\n",
    "We will use the [**London Santander Cycle data**](https://data.london.gov.uk/dataset/number-bicycle-hires) provided by Transport for London (TfL) to experiment with time series data.\n",
    "\n",
    "As reported on the TfL website, the dataset includes the total number of hires of the Santander Cycle Hire Scheme, by day, month, and year for each day since the launch on 30 July 2010.\n",
    "\n",
    "For this session, we are interested only in the daily data (i.e. columns A and B)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mp_LzqXAFirn"
   },
   "outputs": [],
   "source": [
    "#@title Imports\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from calendar import day_abbr, month_abbr, mdays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4O8HtMUpFz9x"
   },
   "source": [
    "## Load data and plot\n",
    "\n",
    "The dataset contains two sheets: `Metadata` and `Data`. Therefore, we need to instruct Pandas on which one we would like to look at, by using the `sheet_name` attribute of the `read_excel` method. Moreover, since there is no need to load data that we are not interested in, it is recommended to specify the columns that we want to use.\n",
    "\n",
    "**NOTE:** The timestamp (which is `Day` in the Excel file) is renamed `datetime` due to a common practice in Pandas. Also, we are selecting the dates from the 1st of Jan 2011 because the climate data that we will be using later in the session is from this date onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f4RCdWoomdrU"
   },
   "outputs": [],
   "source": [
    "# we used the helper function extract_csv_from_excel() to extract the information that we only need from the excel file and turn it to csv\n",
    "extract_csv_from_excel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WxSsDwctIWTc"
   },
   "outputs": [],
   "source": [
    "# we used the load_tfl_cycle_data() function to load the csv file and rename the timestamp\n",
    "cycle_df = load_tfl_cycle_data()\n",
    "cycle_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIt-t684gL_I"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"background-color: #FFCDD2; border: 1px solid red; padding: 10px;\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Task 3.1: Plot time series </b>\n",
    "\n",
    "Plot the cycle data time series.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sXWMay8sJRAC"
   },
   "outputs": [],
   "source": [
    "#### TODO ####\n",
    "#### Plot the cycle data time series ####\n",
    "\n",
    "\n",
    "##############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dXGHLMTKnsLi"
   },
   "source": [
    "## 3.1. Exploratory data analysis\n",
    "\n",
    "Let's now explore the properties of the dataset. Time series data normally comprises three main components:\n",
    "- Trend represents the overall tendency of the data to increase or decrease over time.\n",
    "- Seasonality is related to the presence of recurrent patterns that appear after regular intervals (like seasons).\n",
    "- Random noise is often hard to explain and represents all those changes in the data that seem unexpected. Sometimes sudden changes are related to fixed or predictable events (i.e., public holidays).\n",
    "\n",
    "<div class=\"alert alert-heading alert-danger\" style=\"background-color: white; border: 2px solid; border-radius: 5px; color: #000; border-color:#AAA; padding: 10px\">\n",
    "    <b>üíé Tip</b>\n",
    "\n",
    "* You can go back to 'Lab 4 - Data Processing 2' for references on EDA.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xY-CrqRmpZEA"
   },
   "source": [
    "### 3.1.1 Explore seasonal cycles using a 30-day rolling average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IxTkW_rwpcJt"
   },
   "outputs": [],
   "source": [
    "# Calculate the seasonal cycle by taking a rolling mean with a window of 30 days\n",
    "seasonal_cycle = cycle_df.rolling(window=30, center=True).mean().groupby(cycle_df.index.dayofyear).mean()\n",
    "\n",
    "# Calculate the 25th percentile (q25) of the seasonal cycle\n",
    "q25 = cycle_df.rolling(window=30, center=True).mean().groupby(cycle_df.index.dayofyear).quantile(0.25)\n",
    "\n",
    "# Calculate the 75th percentile (q75) of the seasonal cycle\n",
    "q75 = cycle_df.rolling(window=30, center=True).mean().groupby(cycle_df.index.dayofyear).quantile(0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uHITPY5-uReN"
   },
   "outputs": [],
   "source": [
    "# Modify the element at index 2 to 29, which represents February's number of days in a leap year\n",
    "ndays_m = mdays.copy()\n",
    "ndays_m[2] = 29\n",
    "\n",
    "# Compute the cumulative sum, which gives the cumulative day-of-year values for each month\n",
    "ndays_m = np.cumsum(ndays_m)\n",
    "\n",
    "# Define month ticks\n",
    "month_ticks = month_abbr[1:]\n",
    "\n",
    "# Create a figure and axis for plotting with a specified size\n",
    "f, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "# Plot the 'seasonal_cycle'\n",
    "seasonal_cycle.plot(ax=ax, lw=2, color='b', legend=False)\n",
    "\n",
    "# Fill the area between the 25th and 75th percentiles with a blue color\n",
    "ax.fill_between(seasonal_cycle.index, q25.values.ravel(), q75.values.ravel(), color='b', alpha=0.3)\n",
    "\n",
    "# Set the x-axis tick labels\n",
    "ax.set_xticklabels(month_ticks)\n",
    "\n",
    "# Add gridlines\n",
    "ax.grid(ls=':')\n",
    "\n",
    "# Set the x-axis and y-axis labels\n",
    "ax.set_xlabel('Month', fontsize=15)\n",
    "ax.set_ylabel('Number of Santander cycles hires', fontsize=15)\n",
    "\n",
    "# Set the x-axis and y-axis limits to focus on specific data ranges\n",
    "ax.set_xlim(0, 365)\n",
    "ax.set_ylim(10000, 40000)\n",
    "\n",
    "# Adjust the font sizes of x-axis and y-axis tick labels\n",
    "[l.set_fontsize(13) for l in ax.xaxis.get_ticklabels()]\n",
    "[l.set_fontsize(13) for l in ax.yaxis.get_ticklabels()]\n",
    "\n",
    "# Set the title\n",
    "ax.set_title('30 days running average hourly cycling counts', fontsize=15)\n",
    "\n",
    "# Display\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQ_FBGDAvn7W"
   },
   "source": [
    "### 3.1.2 Explore dependency on year and month via carpet plot/heatmap\n",
    "\n",
    "Heatmaps (also called carpet plots) can give us useful information about the structure of the data. Pandas provides very handy functions to explore relevant dependencies. For example, here we show how the mean Number of Santander Cycles Hires varies as a function of the year and month.\n",
    "\n",
    "Although the number of hires seems to be increasing over time, this increase is not monotonic and probably depends on other factors as well. On the other hand, the carpet plot confirms the general trend shown before, with higher usage of rented bikes over the warmer months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hOEL6Ih8vz7E"
   },
   "outputs": [],
   "source": [
    "# create a copy of the 'cycle_df' DataFrame to avoid modifying the original data\n",
    "month_year = cycle_df.copy()\n",
    "\n",
    "# extract the year and month components from the index and add them as new columns\n",
    "month_year.loc[:,'year'] = month_year.index.year\n",
    "month_year.loc[:,'month'] = month_year.index.month\n",
    "\n",
    "# group the data by year and month, and calculate the mean for each group\n",
    "# then unstack the grouped data to create a pivot table with years as columns and months as rows\n",
    "month_year = month_year.groupby(['year','month']).mean().unstack()\n",
    "\n",
    "# Remove the top-level column index (in this case, the 'year' level)\n",
    "month_year.columns = month_year.columns.droplevel(0)\n",
    "\n",
    "# display the DataFrame\n",
    "month_year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CGCrcWEDwCTt"
   },
   "outputs": [],
   "source": [
    "# create a figure and axis for the heatmap with a specific size\n",
    "f, ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "# create a heatmap plot, specifying the data, axis, colormap, and colorbar settings\n",
    "sns.heatmap(month_year, ax=ax, cmap=plt.cm.viridis, cbar_kws={'boundaries':np.arange(10000,45000,5000)})\n",
    "\n",
    "# get the colorbar axis\n",
    "cbax = f.axes[1]\n",
    "\n",
    "# set the font size for colorbar tick labels\n",
    "[l.set_fontsize(13) for l in cbax.yaxis.get_ticklabels()]\n",
    "\n",
    "# set the colorbar label\n",
    "cbax.set_ylabel('Santander cycles hires', fontsize=13)\n",
    "\n",
    "# add horizontal gridlines\n",
    "[ax.axhline(x, ls=':', lw=0.5, color='0.8') for x in np.arange(1, 7)]\n",
    "\n",
    "# add vertical gridlines\n",
    "[ax.axvline(x, ls=':', lw=0.5, color='0.8') for x in np.arange(1, 24)]\n",
    "\n",
    "# set a title\n",
    "ax.set_title('Santander cycles hires per year and month', fontsize=16)\n",
    "\n",
    "# set the font size for x-axis tick labels\n",
    "[l.set_fontsize(13) for l in ax.xaxis.get_ticklabels()]\n",
    "\n",
    "# set the font size for y-axis tick labels\n",
    "[l.set_fontsize(13) for l in ax.yaxis.get_ticklabels()]\n",
    "\n",
    "# set x-axis label\n",
    "ax.set_xlabel('Month', fontsize=15)\n",
    "\n",
    "# set y-axis label\n",
    "ax.set_ylabel('Year', fontsize=15)\n",
    "\n",
    "# rotate the ticklabels for the y-axis\n",
    "ax.set_yticklabels(np.arange(2011, 2021, 1), rotation=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwCJXakAylzV"
   },
   "source": [
    "### 3.1.3 Explore dependency on day of the week and month via carpet plot/heatmap\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\" style=\"background-color: #FFCDD2; border: 1px solid red; padding: 10px;\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Task 3.2: Heatmap that shows dependencies </b>\n",
    "\n",
    "- Produce a new heatmap showing the dependency on the day of the week and month.\n",
    "- Explore dependency on day of the week and month\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9K8qzI1Bq0xP"
   },
   "outputs": [],
   "source": [
    "# Explore dependency on day of the week and month\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`plot_heatmap()` is a helper function that we wrote for you to make it faster to plot the heatmaps for this particular example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jCZIS9-UyqQC"
   },
   "outputs": [],
   "source": [
    "plot_heatmap(month_day, 'Santander cycles hires per day of the week and month', 'Month', 'Day of the week')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xd1EuNj2ytPe"
   },
   "source": [
    "### 3.1.4 Explore weekdays and weekends trends\n",
    "<div class=\"alert alert-block alert-danger\" style=\"background-color: #FFCDD2; border: 1px solid red; padding: 10px;\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Task 3.3: Differences between weekday and weekend trends </b>\n",
    "\n",
    "- We can also explore deeper trends by looking at differences between weekdays and weekends.\n",
    "- Extract weekday and weekend trends.\n",
    "- Plot weekdays and weekends general trends along with the 25% and 75% IQR to have a sense of the variation over time\n",
    "\n",
    "\n",
    "<div class=\"alert alert-heading alert-danger\" style=\"background-color: white; border: 2px solid; border-radius: 5px; color: #000; border-color:#AAA; padding: 10px\">\n",
    "    <b>üíé Tip</b>\n",
    "\n",
    "* Pandas DateTime indexes have a built-in method that extracts the day of week (`DataFrame.index.dayofweek`), where 0 is Monday and 6 is Sunday.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9vaDyDSKyvC3"
   },
   "outputs": [],
   "source": [
    "## Extract weekdays and weekends trends ##\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_1wcxszRywAX"
   },
   "outputs": [],
   "source": [
    "## Plot weekdays and weekends general trends along with the 25% and 75% IQR to have a sense of the variation over time ##\n",
    "\n",
    "\n",
    "##########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apGz1FAoyyx5"
   },
   "source": [
    "## 3.2. Forecasting using Facebook Prophet\n",
    "\n",
    "Nowadays, there are many models for the predictive analysis of time series data such as ARIMA (**A**uto-**R**egressive **I**ntegrated **M**oving **A**verage model), Auto-Regressive models, Exponential Smoothing, LSTM (**L**ong **S**hort **T**erm **M**emory), etc.\n",
    "\n",
    "Here, we will show how to use a novel approach based on the *Facebook Prophet* library. Fbprophet implements a Generalized Additive Model and models a time series as the sum (or multiplication) of different components (trends, periodic components, holidays, and special events) allowing the incorporation of additional regressors taken from outer sources.\n",
    "The main reference is [Taylor and Letham, 2017](https://peerj.com/preprints/3190.pdf)\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<h4>üë©‚Äçüíª [Optional] - Readings</h4>\n",
    "\n",
    "* The main reference for Fbprophet, [Taylor and Letham, 2017](https://peerj.com/preprints/3190.pdf).\n",
    "* [Official announcement](https://research.fb.com/prophet-forecasting-at-scale/) of Fbprophet.\n",
    "* [Full documentation](https://facebook.github.io/prophet/docs/quick_start.html#python-api) of Fbprophet.\n",
    "\n",
    "</div>\n",
    "\n",
    "The easiest way to install Prophet is through conda-forge. Open the Anaconda Prompt and type: `conda install -c conda-forge fbprophet`.\n",
    "\n",
    "As largely explained in the quick start webpage, Prophet follows the `sklearn` model API. Hence we just need to create an instance of the Prophet class and then call its `fit` and `predict` methods.\n",
    "\n",
    "The input to Prophet is always a data frame with two columns: ds and y. The ds (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp. The y column must be numeric and represents the measurement we wish to forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "shm07A_KzGno"
   },
   "outputs": [],
   "source": [
    "from prophet import Prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxUQQhW2zzZW"
   },
   "source": [
    "### 3.2.1 Base model\n",
    "\n",
    "We first prepare the data in order to be ingested by Prophet. Therefore, we must change the name of the datetime column to `ds` and the target feature to `y`.\n",
    "\n",
    "We will use the helper function `prepare_data()` to do this task. This function takes in the `data` and `target_feature` as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dDvSmnEVz8A4"
   },
   "outputs": [],
   "source": [
    "# create a new DataFrame 'cycle_df_new' by preparing the 'cycle_df' for Facebook Prophet\n",
    "cycle_df_new = prepare_data(data=cycle_df, target_feature='Number of Bicycle Hires')\n",
    "\n",
    "# display the first 5 rows of cycle_df_new\n",
    "cycle_df_new.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqItNf9Pz9wy"
   },
   "source": [
    "#### 3.2.1.1 The holidays package\n",
    "\n",
    "Knowing when holidays and special events take place is often crucial when modelling time-series data. Here we make use of the `holidays` [package](https://github.com/dr-prodigy/python-holidays)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yg8phzPB0CES"
   },
   "outputs": [],
   "source": [
    "import holidays\n",
    "\n",
    "# create an empty DataFrame 'holidays_df' with columns 'ds' and 'holiday'\n",
    "holidays_df = pd.DataFrame([], columns = ['ds','holiday'])\n",
    "\n",
    "# initialize empty lists to store holiday dates and names\n",
    "ldates = []\n",
    "lnames = []\n",
    "\n",
    "# loop through the holidays in the United Kingdom for the specified years (2011 to 2020)\n",
    "for date, name in sorted(holidays.UnitedKingdom(years=np.arange(2011, 2020 + 1)).items()):\n",
    "    # append the holiday date to the 'ldates' list\n",
    "    ldates.append(date)\n",
    "    # append the holiday name to the 'lnames' list\n",
    "    lnames.append(name)\n",
    "\n",
    "# convert the 'ldates' and 'lnames' lists into NumPy arrays\n",
    "ldates = np.array(ldates)\n",
    "lnames = np.array(lnames)\n",
    "\n",
    "# assign the 'ldates' to the 'ds' column and 'lnames' to the 'holiday' column of 'holidays_df'\n",
    "holidays_df.loc[:,'ds'] = ldates\n",
    "holidays_df.loc[:,'holiday'] = lnames\n",
    "\n",
    "# display the unique holiday names in the 'holiday' column\n",
    "holidays_df.holiday.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2CB0H7S40fo_"
   },
   "outputs": [],
   "source": [
    "# remove the string ' (Observed)' from each holiday name\n",
    "# using the apply method with a lambda function to modify the 'holiday' column\n",
    "holidays_df.loc[:,'holiday'] = holidays_df.loc[:,'holiday'].apply(lambda x : x.replace(' (Observed)',''))\n",
    "\n",
    "# display the first 5 rows of the modified 'holidays_df'\n",
    "holidays_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NcqfiOLn0hPd"
   },
   "source": [
    "#### 3.2.1.2 Train test split and model fit\n",
    "\n",
    "We have decided to use the data up to the 31st of July to train the model and the rest (last 2 months) as the holdout test set.\n",
    "\n",
    "You can use the helper function `train_test_split()` to split your data as mentioned above. The function only takes the `data` as input and return both the `train` and `test` sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10b6xVbC0jFp"
   },
   "outputs": [],
   "source": [
    "def train_test_split(data):\n",
    "    # set the 'ds' column as the index and extract rows up to July 31, 2020 for the training set\n",
    "    train = data.set_index('ds').loc[:'2020-07-31', :].reset_index()\n",
    "\n",
    "    # set the 'ds' column as the index and extract rows from August 1, 2020, for the testing set\n",
    "    test = data.set_index('ds').loc['2020-08-01':, :].reset_index()\n",
    "\n",
    "    # return the training and testing sets as seperate DataFrames\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d3np43vj0nSD"
   },
   "outputs": [],
   "source": [
    "# Use the 'train_test_split' function to split the 'cycle_df_new' DataFrame into 'train' and 'test' DataFrames\n",
    "train, test = train_test_split(data=cycle_df_new)\n",
    "\n",
    "# Display the first few rows of the 'train' DataFrame\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQv5FFM50ohQ"
   },
   "source": [
    "We now create a Prophet instance. The model has several parameters, the most important ones being the seasonalities. Here we set all the seasonalities but the daily to True (the data is daily, therefore we can't model in-day cycles).\n",
    "\n",
    "Here we instantiate the simplest Prophet model, but you can set other parameters such as the prior scales for each component of your time-series, as well as the number of Fourier series to use to model the cyclic components.\n",
    "Normally, larger prior scales and a higher order Fourier series will make the model more flexible, but at the cost of a potential overfit. Setting the hyperparameters is of crucial importance and it is normally done via cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "biGMf6BC0p9x"
   },
   "outputs": [],
   "source": [
    "# create a Prophet model (m) with the specified configuration settings:\n",
    "m = Prophet(\n",
    "    holidays=holidays_df,  # using the 'holidays_df' DataFrame to specify holidays\n",
    "    seasonality_mode='multiplicative',  # setting seasonality mode to 'multiplicative'\n",
    "    yearly_seasonality=True,  # enabling yearly seasonality\n",
    "    weekly_seasonality=True,  # enabling weekly seasonality\n",
    "    daily_seasonality=False  # disabling daily seasonality\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6GrtPYe80rnm"
   },
   "outputs": [],
   "source": [
    "# fit the Prophet model 'm' to the training data 'train'\n",
    "m.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zld_zx2K0v1k"
   },
   "source": [
    "The method `make_future_dataframe` creates an extension to the training data in the \"future\". Here our future is obviously the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i27SB-eh0xqq"
   },
   "outputs": [],
   "source": [
    "# use the Prophet model 'm' to generate a 'future' DataFrame with dates for the forecasting period\n",
    "future = m.make_future_dataframe(periods=len(test), freq='1D')\n",
    "\n",
    "# display the last few rows of the 'future' DataFrame\n",
    "future.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2YDBVJT0zWR"
   },
   "source": [
    "The `predict` method produces a comprehensive DataFrame comprising of all the modelled components of the time-series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "56_h6a4S0zvo"
   },
   "outputs": [],
   "source": [
    "# use the Prophet model 'm' to make forecasts for the 'future' DataFrame\n",
    "forecast = m.predict(future)\n",
    "\n",
    "# display the forecasted results\n",
    "forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D6Zmp6Kj03MN"
   },
   "outputs": [],
   "source": [
    "# use the Prophet model 'm' to generate component plots for the 'forecast' data\n",
    "# and display them in a figure\n",
    "f = m.plot_components(forecast, figsize=(12, 16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rheeje2O09dg"
   },
   "source": [
    "In the following tasks, we make use of the functions:\n",
    "- `make_predictions_df`, which is a function to convert the output Prophet dataframe to a datetime index and append the actual target values at the end.\n",
    "- `plot_predictions`, which is a function that plots the predictions.\n",
    "\n",
    "Since the target must be strictly positive (but our model doesn't know it), we clip predictions and confidence bands to have a lower value of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iJUug51y0-gv"
   },
   "outputs": [],
   "source": [
    "# use the 'make_predictions_df' function to make predictions on our forecasted data\n",
    "result = make_predictions_df(forecast, train, test)\n",
    "\n",
    "# update the 'result' DataFrame by clipping 'yhat,' 'yhat_lower,' and 'yhat_upper' values to be no lower than 0\n",
    "result.loc[:,'yhat'] = result.yhat.clip(lower=0)\n",
    "result.loc[:,'yhat_lower'] = result.yhat_lower.clip(lower=0)\n",
    "result.loc[:, 'yhat_upper'] = result.yhat_upper.clip(lower=0)\n",
    "\n",
    "# display the first few rows\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gMXdakob0_92"
   },
   "outputs": [],
   "source": [
    "# use the 'plot_predictions' function to create a plot of the predictions starting from '2019-01-01'\n",
    "f, ax = plot_predictions(result, '2019-01-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAfhnljM1KZ3"
   },
   "source": [
    "At first glance, we can see that the model is not performing well. That is, it can capture the overall trend of the data in terms of annual and weekly oscillations but is not able to generalise well on unseen data. We can spot the poor performance by employing a joint plot.\n",
    "\n",
    "We make use of the `create_joint_plot()` helper function to plot this. It takes the `forecast` (forecasted data) as input, as well as labels for the x and y axes, and a title for the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uo1SIWtT1NKO"
   },
   "outputs": [],
   "source": [
    "# create a joint plot for the training set (up to '2020-07-31')\n",
    "create_joint_plot(result.loc[:'2020-07-31', :], title='Train set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3LEkE_vF1PNW"
   },
   "outputs": [],
   "source": [
    "# create a joint plot for the testing set (from '2020-08-01' onwards)\n",
    "create_joint_plot(result.loc['2020-08-01':, :], title='Test set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hkLg79q1m6Y"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<h4>üë©‚Äçüíª [OPTIONAL] 3.2.2 Model with extra-regressors: incorporating the effect of climate conditions (up to the student)</h4>\n",
    "\n",
    "The data is not enough to model the temporal evolution of the number of Santander Cycles Hires. Data scientists should be able to gather external data that they think might be useful for the problem at hand. In this case, is quite clear that weather conditions should strongly influence the amount of Londoners renting bikes.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KSolAtMT4FfY"
   },
   "outputs": [],
   "source": [
    "# read the DataFrame from a pickled file and assign it to 'df_weather'\n",
    "df_weather = pd.read_pickle('./lab7-data/tfl-data/LondonWeatherHourly.pkl')\n",
    "\n",
    "# display the DataFrame\n",
    "df_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUZ10KPp2RW1"
   },
   "source": [
    "The weather data from a station located in NW3 is recorded every hour and reports the average temperature, wind speed, rain, and cloud cover. For our purposes, we have to resample the data daily. Be careful though, bike rentals mostly happen during the daytime, when people commute between home and work or other places. Therefore, we must label the daytime hours before resampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffbKJo1zrjQF"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"background-color: #FFCDD2; border: 1px solid red; padding: 10px;\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Task 3.4: Creating a new column with is_daytime_hour() function </b>\n",
    "\n",
    "We provide a simple helper function called `is_daytime_hour()` that labels as 1 the hours between 6 am and 8 pm and 0 otherwise. Use the `.apply` method in Pandas to create a new column in the DataFrame called `is_daytime_hour`. Then drop all the rows where the label is 0, because we will not consider those in the resampling process.\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wUKdEAMr2Sd9"
   },
   "outputs": [],
   "source": [
    "def is_daytime_hour(datetime):\n",
    "    # check if the hour component falls between 6 and 20 (inclusive)\n",
    "    if 6 <= datetime.hour <= 20:\n",
    "        return 1 # return 1 if it's a daytime hour\n",
    "    else:\n",
    "        return 0 # return 0 it it's not a daytime hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h_manMCh2TPQ"
   },
   "outputs": [],
   "source": [
    "### TODO ###\n",
    "\n",
    "\n",
    "##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_vgHoRo2UEK"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"background-color: #FFCDD2; border: 1px solid red; padding: 10px;\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Task 3.5: Resampling the data </b>\n",
    "\n",
    "Now, we can resample the data on a daily basis using the well-known `.resample` method in Pandas.\n",
    "\n",
    "Resample the data on a daily basis. Check that the length of the resampled DataFrame matches the length of the Cycle DataFrame (3561 rows)\n",
    "\n",
    "**NOTE** Features like temperature, wind speed and cloud cover can be averaged over the day when resampling. Whereas the precipation is cumulative over the day and taking the average value would not be a reasonable choice.\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l7QlMRNb2U8_"
   },
   "outputs": [],
   "source": [
    "# Resample the data on a daily basis. Check that the length of the resampled DataFrame matches the\n",
    "# length of the Cycle DataFrame (3561 rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVGI2r9H2WFd"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"background-color: #FFCDD2; border: 1px solid red; padding: 10px;\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Task 3.6: Concatenate the original data with the weather data </b>\n",
    "\n",
    "Concatenate the `cycle_df_new` DataFrame obtained earlier with the resampled weather data.\n",
    "\n",
    "<div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UrqKMQ1z2XBM"
   },
   "outputs": [],
   "source": [
    "# Concatenate the 'cycle_df_new' DataFrame with the selected columns from the 'df_weather_resampled' DataFrame,\n",
    "# and reset the index to create 'df_with_weather'\n",
    "\n",
    "\n",
    "############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9dkIvQ7h2YLe"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"background-color: #FFCDD2; border: 1px solid red; padding: 10px;\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Task 3.7: Split the data and instantiate new model </b>\n",
    "\n",
    "Split the new data into training and test sets. Then instantiate a new Prophet model\n",
    "\n",
    "<div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dFHruOlY2Z8t"
   },
   "outputs": [],
   "source": [
    "## Train-test split\n",
    "\n",
    "\n",
    "###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ghOhpBRR2a6s"
   },
   "outputs": [],
   "source": [
    "# Instantiate a new Prophet model\n",
    "\n",
    "############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SpU3a0E92cEp"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"background-color: #FFCDD2; border: 1px solid red; padding: 10px;\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Task 3.8: Add regressors </b>\n",
    "\n",
    "We now want to inform the model that extra-regressors are to be added. The `.add_regressor` method is a simple way of adding extra regressors. See [documentation](https://facebook.github.io/prophet/docs/seasonality,_holiday_effects,_and_regressors.html#additional-regressors) for the usage.\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CEojWbac2c12"
   },
   "outputs": [],
   "source": [
    "# Add the 4 additional regressors to the model with multiplicative mode, namely 'temp', 'wind_speed', 'rain_1h' and 'clouds_all'\n",
    "\n",
    "############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WJFsx0D2d6r"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"background-color: #FFCDD2; border: 1px solid red; padding: 10px;\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Task 3.9: Fit the model </b>\n",
    "\n",
    "Fit the model to the new training set.\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7COs4P_O2ewv"
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "\n",
    "\n",
    "#############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4sxG95m2gPu"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"background-color: #FFCDD2; border: 1px solid red; padding: 10px;\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Task 3.10: Future DataFrame </b>\n",
    "\n",
    "Make the future DataFrame with dates for making the predictions\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g777bh1_2hRt"
   },
   "outputs": [],
   "source": [
    "# Make the future DataFrame\n",
    "\n",
    "############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "khAqJsil2iiY"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"background-color: #FFCDD2; border: 1px solid red; padding: 10px;\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Task 3.11: Add the regressors to the DataFrame </b>\n",
    "\n",
    "The `make_future_dataframe` method by default creates only the `ds` column. Since there are additional regressors we must append them on it. Create a new df called `futures` which includes the 4 additional regressors.\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VEozmc-H2jU7"
   },
   "outputs": [],
   "source": [
    "#Create a dataframe called futures which includes the 4 additional regressors\n",
    "\n",
    "\n",
    "##############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnyxSLi52kkW"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"background-color: #FFCDD2; border: 1px solid red; padding: 10px;\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Task 3.12: Forecast and plot </b>\n",
    "\n",
    "Now we can forecast using the `predict` method as before and plot the componenents of the new model. As expected, you will notice that the extra-regressors are quite influential on the overall trend.\n",
    "You will need to:\n",
    "- forecast the data.\n",
    "- plot the components.\n",
    "\n",
    "<div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TXjewggp2l6d"
   },
   "outputs": [],
   "source": [
    "### Forecast ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GwjNpX1O2nzq"
   },
   "outputs": [],
   "source": [
    "### Plot components ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jupyQRGR2ptk"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"background-color: #FFCDD2; border: 1px solid red; padding: 10px;\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Task 3.13: Plot the predictions </b>\n",
    "\n",
    "Produce a plot of the predictions. Remember to clip the `yhat`, `yhat_lower` and `yhat_upper` columns to 0. You will notice that the extra regressors help locate additional fluctuations in the data. However, if you produce a joint plot you will see that the performance on the test set is still not enough (although strongly improved).\n",
    "\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VffM_Nqt2qkU"
   },
   "outputs": [],
   "source": [
    "##  Produce predictions and plot ###\n",
    "\n",
    "\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vog7MBd_2rw0"
   },
   "outputs": [],
   "source": [
    "# create a joint plot for the training set (up to '2020-07-31')\n",
    "create_joint_plot(result.loc[:'2020-07-31', :], title='Train set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5cIprt3P2s4g"
   },
   "outputs": [],
   "source": [
    "# create a joint plot for the testing set (from '2020-08-01' onwards)\n",
    "create_joint_plot(result.loc['2020-08-01':, :], title='Test set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kvi49rgm2xBq"
   },
   "source": [
    "## 3.2.3 Incorporate the effect of a pandemic\n",
    "\n",
    "Choosing the right features and hyperparameters can be more an art than a science. We here want to show you a third possible model that incorporates a yearly seasonality if a specific year is affected by a pandemic or not.\n",
    "\n",
    "We use the helper function `is_pandemic_affected()` that returns True when the year is a pandemic year and False otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8fgw389a2zxn"
   },
   "outputs": [],
   "source": [
    "# create a copy of the 'df_with_weather' DataFrame\n",
    "df_with_weather_covid = df_with_weather.copy()\n",
    "\n",
    "# create a 'pandemic_affected' column by using the 'is_pandemic_affected' function\n",
    "df_with_weather_covid['pandemic_affected'] = df_with_weather_covid['ds'].apply(is_pandemic_affected)\n",
    "\n",
    "# create a 'not_pandemic_affected' column by negating the result of using 'is_pandemic_affected'\n",
    "df_with_weather_covid['not_pandemic_affected'] = ~df_with_weather_covid['ds'].apply(is_pandemic_affected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GJadu41W20le"
   },
   "outputs": [],
   "source": [
    "# display\n",
    "df_with_weather_covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EzBKds2S212d"
   },
   "outputs": [],
   "source": [
    "# perform a train-test split on the 'df_with_weather_covid' DataFrame\n",
    "train_weather_covid, test_weather_covid = train_test_split(df_with_weather_covid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKI8TUJ522_C"
   },
   "source": [
    "Instantiate new Prophet model with yearly_seasonality set to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y5o6cQo523uL"
   },
   "outputs": [],
   "source": [
    "# initialize a Prophet model for time series forecasting with specific configuration\n",
    "m = Prophet(holidays=holidays_df,\n",
    "            seasonality_mode='multiplicative',\n",
    "            yearly_seasonality=False,\n",
    "            weekly_seasonality=True,\n",
    "            daily_seasonality=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KROs3qXO25Bl"
   },
   "outputs": [],
   "source": [
    "# add the 4 additional regressors to the model with multiplicative mode, namely 'temp', 'wind_speed', 'rain_1h' and 'clouds_all'\n",
    "m.add_regressor('temp', mode='multiplicative')\n",
    "m.add_regressor('wind_speed', mode='multiplicative')\n",
    "m.add_regressor('rain_1h', mode='multiplicative')\n",
    "m.add_regressor('clouds_all', mode='multiplicative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kTofiM6T254Y"
   },
   "outputs": [],
   "source": [
    "# add a custom seasonality for 'pandemic_affected' periods\n",
    "m.add_seasonality(name='pandemic_affected', period=365, fourier_order=3, mode='multiplicative', condition_name='pandemic_affected')\n",
    "\n",
    "# add a custom seasonality for 'not_pandemic_affected' periods\n",
    "m.add_seasonality(name='not_pandemic_affected', period=365, fourier_order=3, mode='multiplicative', condition_name='not_pandemic_affected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PhSwwOOT26wH"
   },
   "outputs": [],
   "source": [
    "# fit the Prophet model with the training data, including the additional regressors and custom seasonalities\n",
    "m.fit(train_weather_covid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xdyWX-_o28I3"
   },
   "outputs": [],
   "source": [
    "# create a 'future' DataFrame with dates for making predictions, spanning the length of the testing data\n",
    "future = m.make_future_dataframe(periods=len(test_weather_covid), freq='1D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1dyAaLDa28_O"
   },
   "outputs": [],
   "source": [
    "# concatenate the 'future' DataFrame with selected columns from 'df_weather_resampled' to include additional regressors\n",
    "futures = pd.concat([future, df_weather_resampled.loc[:, ['temp', 'wind_speed', 'rain_1h', 'clouds_all']].reset_index()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00vLJiEv2-QY"
   },
   "outputs": [],
   "source": [
    "# create a 'pandemic_affected' column using the 'is_pandemic_affected' function\n",
    "futures['pandemic_affected'] = futures['ds'].apply(is_pandemic_affected)\n",
    "\n",
    "# create a 'not_pandemic_affected' column by negating the result of using 'is_pandemic_affected'\n",
    "futures['not_pandemic_affected'] = ~futures['ds'].apply(is_pandemic_affected)\n",
    "\n",
    "# display the first 5 rows\n",
    "futures.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dlMf_xaF2_Nm"
   },
   "outputs": [],
   "source": [
    "# use the trained Prophet model to make predictions for the 'futures' DataFrame\n",
    "forecast = m.predict(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NsGbD6um2_6y"
   },
   "outputs": [],
   "source": [
    "# prepare the predictions DataFrame and clip the values to be non-negative\n",
    "result = make_predictions_df(forecast, train_weather_covid, test_weather_covid)\n",
    "\n",
    "# clip the 'yhat' values to be non-negative\n",
    "result.loc[:,'yhat'] = result.yhat.clip(lower=0)\n",
    "\n",
    "# clip the 'yhat_lower' values to be non-negative\n",
    "result.loc[:,'yhat_lower'] = result.yhat_lower.clip(lower=0)\n",
    "\n",
    "# clip the 'yhat_upper' values to be non-negative\n",
    "result.loc[:, 'yhat_upper'] = result.yhat_upper.clip(lower=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "klhfjATG3A1N"
   },
   "outputs": [],
   "source": [
    "# create a plot of predictions using the 'plot_predictions' function, starting from '2019-01-01'\n",
    "f, ax = plot_predictions(result, '2019-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SoHX5V--3A73"
   },
   "outputs": [],
   "source": [
    "# create a joint plot for the training set up to '2020-07-31'\n",
    "create_joint_plot(result.loc[:'2020-07-31', :], title='Train set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T3o8vutG3DqH"
   },
   "outputs": [],
   "source": [
    "# create a joint plot for the test set starting from '2020-08-01'\n",
    "create_joint_plot(result.loc['2020-08-01':, :], title='Test set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4A_Rm_vj3FLc"
   },
   "source": [
    "Although the performance has not improved in terms of $R^2$, we can see that the information about the COVID-19 pandemic made the model able to discern that in 2020 the behavior is different from the previous years. In fact, by looking at the `pandemic_affected` component we can observe several things:\n",
    "\n",
    "- there is an unexpected and sudden drop in March and April, not present in the previous years.\n",
    "- strong increase in the warmer months. This can be related to multiple causes such as the particularly sunny summer, the relaxation of restrictions across the UK, people avoiding public transport, and the major changes to the cycling infrastructure made in London."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eiUS3MTN3Gvz"
   },
   "outputs": [],
   "source": [
    "# create a figure to plot the individual components of the forecast\n",
    "f = m.plot_components(forecast, figsize=(12, 18))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

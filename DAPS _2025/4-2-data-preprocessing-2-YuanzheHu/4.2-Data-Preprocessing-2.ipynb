{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div width=50% style=\"display: block; margin: auto\">\n",
    "    <img src=\"figures/ucl-logo.svg\" width=100%>\n",
    "</div>\n",
    "\n",
    "### [UCL-ELEC0136 Data Acquisition and Processing Systems 2024]()\n",
    "University College London\n",
    "# Lab 4.2: Data Processing #2 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Objectives\n",
    "The data processing 2 lab focuses on Exploratory Data Analysis (EDA) techniques and addressing data imbalance through fairness postprocessing methods. In particular, we will\n",
    "* Learn how to plot and interpret insights from plots\n",
    "* Briefly cover the steps of (1) data transformation, (2) generating data splits, (3) selecting baseline models and building a classifier\n",
    "* Learn how to produce and evaluate the models for performance and fairness\n",
    "* Learn how to leverage fairness metrics for threshold optimization.\n",
    "\n",
    "\n",
    "### Outline\n",
    "0. Setup\n",
    "1. Data Processing\n",
    "2. Train Baselines and a Classifier (Offline)\n",
    "3. Evaluate and Audit for Fairness the Baselines and the Classifier\n",
    "4. Postprocessing Techniques for Fairness\n",
    "\n",
    "<hr width=70% style=\"float: left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.1 Prerequisites\n",
    "This lab uses the following packages:\n",
    "\n",
    "* `folktables`: datasets derived from the American Community Survey (ACS)  Public Use Microdata Sample (PUMS) files which are managed by the US Census Bureau <!-- - more info [here](https://github.com/socialfoundations/folktables)) -->\n",
    "* `scikit-learn`: toolkit for predictive data analysis\n",
    "* `pandas`: data analysis and manipulation toolkit\n",
    "* `matplotlib`: statistical visualization library\n",
    "* `seaborn`: statistical visualization library\n",
    "* `fairlearn`: toolkit for assessing and mitigating an ML system's biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<h4>üë®üèæ‚Äçüíªüë®üèø‚Äçüíª TASK 1: Install Prerequisites</h4>\n",
    "\n",
    "Use the `requirements.txt` file for installing the libraries discussed above.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the following libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for data analysis and manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Libraries for visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.2 Data Acquisition\n",
    "\n",
    "`folktables` offers predefined predictive tasks and the ability to specify the ML problem you want to address. While we won't delve into the thechnical implementation here, the curious minds can explore the details in the `utils/dataset.py` file.\n",
    "\n",
    "In this lab, we specifically focus on **employment status classification** based on 15 input features such as gender, race, marital status, and age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset import ACSDataset\n",
    "\n",
    "# Selected target - Employment status record\n",
    "target_task='ESR'\n",
    "\n",
    "# Sensitive group\n",
    "sensitive_group= 'RAC1P'\n",
    "\n",
    "# Pick a state to download data from available_states list.\n",
    "state_list=['CA']\n",
    "\n",
    "# Pick data duration. Available options: [\"1-Year\", \"5-Year\"].\n",
    "duration='1-Year'\n",
    "\n",
    "# Pick data year. Available options: [\"2015\", \"2016\", \"2017\", \"2018\"].\n",
    "year='2018'\n",
    "\n",
    "# Pick data granularity. Available options: [\"person\", \"household\"].\n",
    "granularity= 'person'\n",
    "\n",
    "ACSProblem=ACSDataset(target_task,sensitive_group,state_list,duration,year,granularity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data for 2018 1-Year person survey for CA...\n"
     ]
    }
   ],
   "source": [
    "dataset_df=ACSProblem.acquire_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Processing  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 (Initial) Data Exploration & Data Cleaning\n",
    "Next we will investigate, analyze and summarize the characteristics of the acquired dataset. This is valuable for\n",
    "\n",
    "1. spotting errors within the dataset;\n",
    "2. grasping data patterns and relationships across variables; and\n",
    "3. identifying outliers or irregular behaviors,\n",
    "\n",
    "and dealing with these issues. \n",
    "\n",
    "First, let's take an initial look at our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape (rows,columns) is  (195665, 17)\n",
      "\n",
      "\n",
      "Dataset Overview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGEP</th>\n",
       "      <th>SCHL</th>\n",
       "      <th>MAR</th>\n",
       "      <th>RELP</th>\n",
       "      <th>DIS</th>\n",
       "      <th>ESP</th>\n",
       "      <th>CIT</th>\n",
       "      <th>MIG</th>\n",
       "      <th>MIL</th>\n",
       "      <th>ANC</th>\n",
       "      <th>NATIVITY</th>\n",
       "      <th>DEAR</th>\n",
       "      <th>DEYE</th>\n",
       "      <th>DREM</th>\n",
       "      <th>SEX</th>\n",
       "      <th>RAC1P</th>\n",
       "      <th>ESR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>27.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>19.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>19.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AGEP  SCHL  MAR  RELP  DIS  ESP  CIT  MIG  MIL  ANC  NATIVITY  DEAR  DEYE  \\\n",
       "0  30.0  14.0  1.0  16.0  2.0  0.0  1.0  3.0  4.0  1.0       1.0   2.0   2.0   \n",
       "1  21.0  16.0  5.0  17.0  2.0  0.0  1.0  1.0  4.0  2.0       1.0   2.0   2.0   \n",
       "2  65.0  22.0  5.0  17.0  2.0  0.0  1.0  1.0  4.0  1.0       1.0   2.0   2.0   \n",
       "3  33.0  14.0  3.0  16.0  2.0  0.0  1.0  1.0  4.0  1.0       1.0   2.0   2.0   \n",
       "4  18.0  19.0  5.0  17.0  1.0  0.0  1.0  1.0  4.0  2.0       1.0   2.0   2.0   \n",
       "5  17.0  15.0  5.0  16.0  2.0  0.0  1.0  3.0  4.0  2.0       1.0   2.0   2.0   \n",
       "6  27.0  21.0  5.0  17.0  2.0  0.0  1.0  1.0  1.0  3.0       1.0   2.0   2.0   \n",
       "7  19.0  18.0  5.0  17.0  2.0  0.0  1.0  3.0  4.0  4.0       1.0   2.0   2.0   \n",
       "8  27.0  19.0  5.0  17.0  2.0  0.0  1.0  1.0  4.0  2.0       1.0   2.0   2.0   \n",
       "9  19.0  16.0  5.0  17.0  2.0  0.0  1.0  3.0  1.0  1.0       1.0   2.0   2.0   \n",
       "\n",
       "   DREM  SEX  RAC1P  ESR  \n",
       "0   2.0  1.0    8.0  0.0  \n",
       "1   2.0  1.0    1.0  1.0  \n",
       "2   2.0  1.0    1.0  1.0  \n",
       "3   2.0  1.0    1.0  0.0  \n",
       "4   1.0  2.0    1.0  1.0  \n",
       "5   2.0  2.0    9.0  0.0  \n",
       "6   2.0  1.0    1.0  0.0  \n",
       "7   2.0  2.0    2.0  0.0  \n",
       "8   2.0  1.0    1.0  1.0  \n",
       "9   2.0  1.0    1.0  0.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the dataset shape (rows,columns)\n",
    "print('Dataset shape (rows,columns) is ', dataset_df.shape)\n",
    "print('\\n')\n",
    "\n",
    "# Get an overview of the dataset by printing the first 10 rows\n",
    "print('Dataset Overview:')\n",
    "dataset_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information about the dataset:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 195665 entries, 0 to 195664\n",
      "Data columns (total 17 columns):\n",
      " #   Column    Non-Null Count   Dtype  \n",
      "---  ------    --------------   -----  \n",
      " 0   AGEP      195665 non-null  float64\n",
      " 1   SCHL      195665 non-null  float64\n",
      " 2   MAR       195665 non-null  float64\n",
      " 3   RELP      195665 non-null  float64\n",
      " 4   DIS       195665 non-null  float64\n",
      " 5   ESP       195665 non-null  float64\n",
      " 6   CIT       195665 non-null  float64\n",
      " 7   MIG       195665 non-null  float64\n",
      " 8   MIL       195665 non-null  float64\n",
      " 9   ANC       195665 non-null  float64\n",
      " 10  NATIVITY  195665 non-null  float64\n",
      " 11  DEAR      195665 non-null  float64\n",
      " 12  DEYE      195665 non-null  float64\n",
      " 13  DREM      195665 non-null  float64\n",
      " 14  SEX       195665 non-null  float64\n",
      " 15  RAC1P     195665 non-null  float64\n",
      " 16  ESR       195665 non-null  float64\n",
      "dtypes: float64(17)\n",
      "memory usage: 25.4 MB\n"
     ]
    }
   ],
   "source": [
    "# Print the index dtype and columns, non-null values and memory usage\n",
    "print('Information about the dataset:')\n",
    "dataset_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<h4>üë©üèº‚Äçüíªüë©üèΩ‚Äçüíª Task 1.1: Deal with erroneous data types</h4>\n",
    "\n",
    "Erroneous data types occur when the format or the categorization of the data type does not align with the intended or expected type for a given attribute or variable. \n",
    "The basic overview of the data above shows that every column is a numerical.\n",
    "\n",
    "Fix this issue as follows:\n",
    "\n",
    "* Create a function `cast_features(categorical_features: list[str], numerical_features: list[str], data:pd.DataFrame)` that fixes the erroneous datatypes of the dataframe\n",
    "* The function should return a pandas dataframe containing the updated data\n",
    "  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-heading alert-danger\" style=\"background-color: white; border: 2px solid; border-radius: 5px; color: #000; border-color:#AAA; padding: 10px\">\n",
    "<b>üíé Tips</b> \n",
    "\n",
    "* The target should not be part of the input features\n",
    "* Use `ACSProblem.categorical_features` and `ACSProblem.numerical_features` to find the categorical and numerical features of this dataset.\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_features(categorical_features: list[str], numerical_features: list[str], data:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cast features to the correct data type.\n",
    "\n",
    "    Args:\n",
    "        categorical_features (List[str]): list of categorical feature names\n",
    "        numerical_features (List[str]): list of numerical feature names\n",
    "        data (pd.DataFrame): raw dataframe with unprocessed data\n",
    "    Returns:\n",
    "        (pd.DataFrame): a dataframe with the casted features data\n",
    "    \"\"\"\n",
    "\n",
    "    # Cast categorical features to category data type\n",
    "    data[categorical_features] = data[categorical_features].astype('category')\n",
    "\n",
    "    # Cast numerical features to float data type\n",
    "    data[numerical_features] = data[numerical_features].astype('float')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have fixed the erroneous data types, let's check for missing values! We will start by identyifying the features with missing values and the quantity of these missing values in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n"
     ]
    }
   ],
   "source": [
    "# Print the missing values per column\n",
    "print('Missing values per column:')\n",
    "missing_values = dataset_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our dataset has no missing values, let's move on to generate and review the descriptive statistics of the features and the target. These will help us understand the distribution of both features and the target. Additionally, they will help us determine the most suitable visualization method for each column in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptive statistics for dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGEP</th>\n",
       "      <th>SCHL</th>\n",
       "      <th>MAR</th>\n",
       "      <th>RELP</th>\n",
       "      <th>DIS</th>\n",
       "      <th>ESP</th>\n",
       "      <th>CIT</th>\n",
       "      <th>MIG</th>\n",
       "      <th>MIL</th>\n",
       "      <th>ANC</th>\n",
       "      <th>NATIVITY</th>\n",
       "      <th>DEAR</th>\n",
       "      <th>DEYE</th>\n",
       "      <th>DREM</th>\n",
       "      <th>SEX</th>\n",
       "      <th>RAC1P</th>\n",
       "      <th>ESR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>195665.000000</td>\n",
       "      <td>195665.000000</td>\n",
       "      <td>195665.000000</td>\n",
       "      <td>195665.000000</td>\n",
       "      <td>195665.000000</td>\n",
       "      <td>195665.000000</td>\n",
       "      <td>195665.000000</td>\n",
       "      <td>195665.000000</td>\n",
       "      <td>195665.000000</td>\n",
       "      <td>195665.000000</td>\n",
       "      <td>195665.000000</td>\n",
       "      <td>195665.000000</td>\n",
       "      <td>195665.000000</td>\n",
       "      <td>195665.000000</td>\n",
       "      <td>195665.000000</td>\n",
       "      <td>195665.000000</td>\n",
       "      <td>195665.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>42.734914</td>\n",
       "      <td>18.470309</td>\n",
       "      <td>2.654343</td>\n",
       "      <td>2.510403</td>\n",
       "      <td>1.945836</td>\n",
       "      <td>0.014172</td>\n",
       "      <td>2.100253</td>\n",
       "      <td>1.272389</td>\n",
       "      <td>3.894028</td>\n",
       "      <td>1.677234</td>\n",
       "      <td>1.312340</td>\n",
       "      <td>1.984402</td>\n",
       "      <td>1.988761</td>\n",
       "      <td>1.984029</td>\n",
       "      <td>1.472001</td>\n",
       "      <td>3.072675</td>\n",
       "      <td>0.880449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14.884622</td>\n",
       "      <td>3.942463</td>\n",
       "      <td>1.846382</td>\n",
       "      <td>4.445350</td>\n",
       "      <td>0.226342</td>\n",
       "      <td>0.270206</td>\n",
       "      <td>1.612849</td>\n",
       "      <td>0.681428</td>\n",
       "      <td>0.465836</td>\n",
       "      <td>1.039665</td>\n",
       "      <td>0.463449</td>\n",
       "      <td>0.123915</td>\n",
       "      <td>0.105415</td>\n",
       "      <td>0.125364</td>\n",
       "      <td>0.499217</td>\n",
       "      <td>2.915503</td>\n",
       "      <td>0.324437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>42.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>55.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>94.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                AGEP           SCHL            MAR           RELP  \\\n",
       "count  195665.000000  195665.000000  195665.000000  195665.000000   \n",
       "mean       42.734914      18.470309       2.654343       2.510403   \n",
       "std        14.884622       3.942463       1.846382       4.445350   \n",
       "min        17.000000       1.000000       1.000000       0.000000   \n",
       "25%        30.000000      16.000000       1.000000       0.000000   \n",
       "50%        42.000000      19.000000       1.000000       1.000000   \n",
       "75%        55.000000      21.000000       5.000000       2.000000   \n",
       "max        94.000000      24.000000       5.000000      17.000000   \n",
       "\n",
       "                 DIS            ESP            CIT            MIG  \\\n",
       "count  195665.000000  195665.000000  195665.000000  195665.000000   \n",
       "mean        1.945836       0.014172       2.100253       1.272389   \n",
       "std         0.226342       0.270206       1.612849       0.681428   \n",
       "min         1.000000       0.000000       1.000000       1.000000   \n",
       "25%         2.000000       0.000000       1.000000       1.000000   \n",
       "50%         2.000000       0.000000       1.000000       1.000000   \n",
       "75%         2.000000       0.000000       4.000000       1.000000   \n",
       "max         2.000000       8.000000       5.000000       3.000000   \n",
       "\n",
       "                 MIL            ANC       NATIVITY           DEAR  \\\n",
       "count  195665.000000  195665.000000  195665.000000  195665.000000   \n",
       "mean        3.894028       1.677234       1.312340       1.984402   \n",
       "std         0.465836       1.039665       0.463449       0.123915   \n",
       "min         1.000000       1.000000       1.000000       1.000000   \n",
       "25%         4.000000       1.000000       1.000000       2.000000   \n",
       "50%         4.000000       1.000000       1.000000       2.000000   \n",
       "75%         4.000000       2.000000       2.000000       2.000000   \n",
       "max         4.000000       4.000000       2.000000       2.000000   \n",
       "\n",
       "                DEYE           DREM            SEX          RAC1P  \\\n",
       "count  195665.000000  195665.000000  195665.000000  195665.000000   \n",
       "mean        1.988761       1.984029       1.472001       3.072675   \n",
       "std         0.105415       0.125364       0.499217       2.915503   \n",
       "min         1.000000       1.000000       1.000000       1.000000   \n",
       "25%         2.000000       2.000000       1.000000       1.000000   \n",
       "50%         2.000000       2.000000       1.000000       1.000000   \n",
       "75%         2.000000       2.000000       2.000000       6.000000   \n",
       "max         2.000000       2.000000       2.000000       9.000000   \n",
       "\n",
       "                 ESR  \n",
       "count  195665.000000  \n",
       "mean        0.880449  \n",
       "std         0.324437  \n",
       "min         0.000000  \n",
       "25%         1.000000  \n",
       "50%         1.000000  \n",
       "75%         1.000000  \n",
       "max         1.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate descriptive statistics about your dataset\n",
    "print('Descriptive statistics for dataset:')\n",
    "dataset_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Exploratory Data Analysis: Visualisations\n",
    "\n",
    "\n",
    "**1. Target distribution visualization**\n",
    "\n",
    "Let's analyze the distribution of samples acfoss each target label. Since our target task (i.e., Employment Status Record - `ESR`) is categorical, an effective method to visualize the target labels is through bar plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a bar plot to visualize the targets\n",
    "df[ACSProblem.target_task].value_counts().sort_values().plot(kind='bar')\n",
    "\n",
    "# Add axes labels\n",
    "plt.ylabel('Number of samples')\n",
    "plt.xlabel('Employment Status Record - ESR')\n",
    "\n",
    "# Show Figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b> Q: What insights can you derive from the bar plot of the target? </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your answer here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Target and sensitive group distribution visualization**\n",
    "\n",
    "Now, let's see the proportion of the sensitive group (i.e., Race - `RAC1P`) per target label. For simplicity we include the races from the [original paper](https://arxiv.org/abs/2108.04884) here:\n",
    "\n",
    "* 1: White alone\n",
    "* 2: Black or African American alone\n",
    "* 3: American Indian alone\n",
    "* 4: Alaska Native alone\n",
    "* 5: American Indian and Alaska Native tribes specified, or American Indian or Alaska Native, not specified and no other races\n",
    "* 6: Asian alone\n",
    "* 7: Native Hawaiian and Other Pacific Islander alone\n",
    "* 8: Some Other Race alone\n",
    "* 9: Two or More Races"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping based on target and sensitive group and plot\n",
    "df.groupby([ACSProblem.target_task, ACSProblem.sensitive_group])[ACSProblem.sensitive_group].count().unstack().plot(\n",
    "    kind=\"bar\", stacked=True)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(title='Race')\n",
    "\n",
    "# Add axes labels\n",
    "plt.ylabel('Number of samples')\n",
    "plt.xlabel('Employment Status Record - ESR')\n",
    "\n",
    "# Show Figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b> Q: What insights can you derive from the bar plot? </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your answer here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<h4>üë©üèæ‚Äçüíªüë®üèª‚Äçüíª Task 2.1: Find an appropriate visualization for the sensitive groups x targets </h4>\n",
    "\n",
    "The previous plot allows to investigate a general trend of the sensitive group allocation on each class, but it does not allow to understand what the exact amount of the underrepresented sensitive groups.\n",
    "\n",
    "* Decide an appropriate plot that allows to effectively display the proportion of each target class per sensitive group. \n",
    "* Create a function `sensitive_group_plot(data:pd.DataFrame, sensitive: str, target: str)` that generates the plot. \n",
    "* The function must both the `Figure` and the `Axis` objects.\n",
    "  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-heading alert-danger\" style=\"background-color: white; border: 2px solid; border-radius: 5px; color: #000; border-color:#AAA; padding: 10px\">\n",
    "    <b>üíé Tip</b>\n",
    "\n",
    "* The folowing plot includes the Race (i.e., `RAC1P`) memberships on x-axis, the amount of the samples with this membership on y-axis and the target category using colors.\n",
    "\n",
    "* This is **not a good visualization because some groups (e.g., group 4 and 5) are not displayed efficiently** (akin to the one above).\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following example is a bad visualisation!!!\n",
    "# Plot a histogram with stacking bars\n",
    "sns.displot(\n",
    "    df,\n",
    "    x=sensitive_group,\n",
    "    hue=ACSProblem.target_task, \n",
    "    kind=\"hist\",\n",
    "    stat=\"count\",\n",
    "    bins=10,\n",
    "    multiple=\"stack\",\n",
    ")\n",
    "\n",
    "# Add axes labels\n",
    "plt.ylabel('Number of samples')\n",
    "plt.xlabel('Race')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-heading alert-danger\" style=\"background-color: white; border: 2px solid; border-radius: 5px; color: #000; border-color:#AAA; padding: 10px\">\n",
    "<b>üíé Tip</b>\n",
    " \n",
    "* Use `ACSProblem.race_labels`, `ACSProblem.sensitive_group` and `ACSProblem.target_task` to find the race labels dictionary, the sensitive group and the target name column, respectively.\n",
    "\n",
    "* Look into [`plt.subplots`](https://matplotlib.org/stable/gallery/subplots_axes_and_figures/subplots_demo.html) to create multiple plots using a `Figure` and `Axis` objects.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensitive_group_plot(data:pd.DataFrame, race_labels : list[str], sensitive: str, target: str) -> (plt.Figure, plt.Axes):\n",
    "    \"\"\"\n",
    "    Creates a line plot for each list of data in the given list of data, title and labels.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): dataframe including all the avaible data\n",
    "        race_labels (list[str]): the list of races available in the dataset\n",
    "        sensitive (str): the column name of the sensitive attribute\n",
    "        target (str): the column name of the target task\n",
    "        \n",
    "    Returns:\n",
    "        (plt.Figure, plt.Axes): matplotlib figure and axes objects\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add your code here:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<h4>üë®üèΩ‚Äçüíªüë©üèª‚Äçüíª [Optional] TASK 2.2: Input Features Visualisations</h4>\n",
    "\n",
    "* Create a function `histograms(data:pd.DataFrame, categorical_features: list[str])` that will plot a histogram for each of the features in the categorical features of the dataset \n",
    "* The function must return both the `Figure` and the `Axis` objects\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histograms(data:pd.DataFrame, categorical_features: list[str], column_name_dic: dict) -> (plt.Figure, plt.Axes):\n",
    "    \"\"\"\n",
    "    Creates a histogram plot for each of the categorical features in the given dataframe.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): dataframe with input features \n",
    "        categorical_features (List[str]): list of categorical feature names\n",
    "    Returns:\n",
    "        (plt.Figure, plt.Axes): matplotlib figure and axes objects\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<h4>üßëüèº‚Äçüíªüë©üèø‚ÄçüíªTASK 2.3: Plotting numerical data</h4>\n",
    "\n",
    "* Decide an appropriate plot that allows to effectively display the numerical features and the proportion of each target class \n",
    "* Create a function `numerical_plot(data:pd.DataFrame, numerical_features: list[str])` that produces the plot you picked\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-heading alert-danger\" style=\"background-color: white; border: 2px solid; border-radius: 5px; color: #000; border-color:#AAA; padding: 10px\">\n",
    "<b>üíé Tip</b>\n",
    " \n",
    "*  Use `ACSProblem.numerical_features` and `ACSProblem.target_task` to get the numerical features and the target task of this ML problem.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_plot(data:pd.DataFrame, numerical_features: list[str], target: str):\n",
    "    \"\"\"\n",
    "    Creates a histogram plot for each of the categorical features in the given dataframe.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): dataframe with input features \n",
    "        categorical_features (List[str]): list of categorical feature names\n",
    "        \n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Data Transformation \n",
    "\n",
    "Since we haven't covered data transformations yet, feel free to consider it a black box and not stress about it. However, if you're curious, you can explore the implementation of the `ACSDataset Class`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ACSProblem' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Perform data transformation\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_trans\u001b[38;5;241m=\u001b[39m\u001b[43mACSProblem\u001b[49m\u001b[38;5;241m.\u001b[39mdata_transforms(df)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ACSProblem' is not defined"
     ]
    }
   ],
   "source": [
    "# Perform data transformation\n",
    "df_trans=ACSProblem.data_transforms(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe transformed data\n",
    "df_trans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train Baselines and a Classifier (Offline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to how we've approached the <b>Data Transformation</b> process, we will handle most of the implementation behind the following steps as black-boxes and we will addresss them in later sessions.\n",
    "\n",
    "<b><span style=\"color: #C0392B\">IMPORTANT NOTE üõë:</span></b> In ELEC0136 week 7, we will learn that during (offline) model training/fitting phase we usually leverage a training set and a validation set. However, `scikit-learn` library does not have build-in support for validation sets. Therefore, in what follows we use only a training set for the (vanilla) training procedure. See more [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Split data into train/test splits\n",
    "First,  let's generate the data splits we are going to use for fitting and evaluating our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data splits\n",
    "train_data, test_data= ACSProblem.generate_splits(df_trans, val_split=False)\n",
    "\n",
    "# Perform column wise partition for each split\n",
    "X_train, sens_train, y_train= ACSProblem.columnwise_partition(train_data)\n",
    "X_test, sens_test, y_test= ACSProblem.columnwise_partition(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Pick a baseline model\n",
    "Next, we pick some simple baseline models that we will use as reference points to evaluate whether the developed model is truly learning patterns and performs better than a random or naive approach. We will use the following baselines:\n",
    "\n",
    "* *Uniformly Random Guess*: Predict the targets with equal probability.\n",
    "* *Prior Random Guess*: Predict 0 or 1 proportional to the prior probability in the dataset.\n",
    "* *Majority Class*: Predict only the the majority/most frequent class, i.e., mode (for our example is class 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import models\n",
    "\n",
    "# Develop the baselines\n",
    "uniform_clf, mode_clf, prior_clf = models.build_baselines(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Pick and train an initial model\n",
    "\n",
    "Let's select logistic regression as our classifier. You can experiment with other models (e.g. an MLP) at a later time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Develop the logistic regression model\n",
    "LR_clf=models.build_LRModel(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Evaluate and Audit for Fairness the Baselines and the Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows we evaluate the performance of the produced baselines and naive model using the following metris:\n",
    "1. Accuracy: measures correct perdictions\n",
    "2. Precision: accuracy of positive predictions\n",
    "3. Recall: measures the ability to correctly identify positive instances\n",
    "4. F1-score: provides a balance between precision and recall, especially when there is an uneven class distribution\n",
    "5. Confusion Matrix:summary of the model's predictions versus the actual classes and includes four metrics: true positive, true negative, false positive, and false negative.\n",
    "6. Demographic Parity Difference: measures the disparity in positive outcomes among sensitive groups.\n",
    "7. Equality of Odds Difference: measures the disparity in true positive and false positive rates across sensitive groups.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for resutls reporting\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score,recall_score, f1_score,ConfusionMatrixDisplay\n",
    "from fairlearn.metrics import equalized_odds_difference, demographic_parity_difference\n",
    "\n",
    "def report_performance(model_list: list[str], X:pd.DataFrame, y:pd.DataFrame, sens:pd.DataFrame):\n",
    "    # Add your code here:\n",
    "\n",
    "    for model in model_list:\n",
    "        y_pred=model.predict(X)\n",
    "        print('-----------------')\n",
    "        print(f\"Performance of {model}\")\n",
    "        print(\"Accuracy:\", accuracy_score(y, y_pred))\n",
    "        print(\"Precision:\", precision_score(y, y_pred))\n",
    "        print(\"Recall:\", recall_score(y, y_pred))\n",
    "        print(\"F1-score:\", f1_score(y, y_pred))\n",
    "        print(\"Confusion Matrix:\", confusion_matrix(y, y_pred))\n",
    "        print(\"Demographic Parity Difference:\", demographic_parity_difference(\n",
    "        y, y_pred, sensitive_features=sens))\n",
    "        print(\"Equalized Odds Difference:\", equalized_odds_difference(\n",
    "        y, y_pred, sensitive_features=sens)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Examine performance on training set\n",
    "report_performance([uniform_clf, mode_clf, prior_clf,LR_clf],X_train,y_train,sens_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Examine performance on test set\n",
    "report_performance([uniform_clf, mode_clf, prior_clf,LR_clf],X_test,y_test,sens_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b> Q: What insights can you derive from the reported metrics for all models? </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your response here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Postprocessing Techniques for Fairness\n",
    "\n",
    "\n",
    "Since we audited the (naive/initial) logistic regression model for fairness and we noticed that it does not satisfy Equalizes Odds, we are going to enforce this fairness metric as a postprocessing technique. \n",
    "\n",
    "To do so, we are going to use `fairlearn` library's [`ThresholdOptimzer`](https://fairlearn.org/v0.9/api_reference/generated/fairlearn.postprocessing.ThresholdOptimizer.html?highlight=thresholdoptimizer#fairlearn.postprocessing.ThresholdOptimizer) to implement Equality of Opportunity. \n",
    "\n",
    "\n",
    "You can find the original paper introducing Equalized Odds [here](https://proceedings.neurips.cc/paper/2016/file/9d2682367c3935defcb1f9e247a97c0d-Paper.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "\n",
    "def build_EOddFairModel(cls,  X:pd.DataFrame, y:pd.DataFrame, sens:pd.DataFrame):\n",
    "    # Set up ThresholdOptimizer\n",
    "    eo_model = ThresholdOptimizer(\n",
    "        estimator=cls,\n",
    "        constraints=\"equalized_odds\", # Optimize FPR and FNR simultaneously\n",
    "        objective=\"balanced_accuracy_score\", # accuracy_score\n",
    "        grid_size=1000,\n",
    "        flip=False,\n",
    "        prefit=False,\n",
    "        predict_method=\"predict_proba\",\n",
    "    )\n",
    "    \n",
    "    # Adjust the results that the classifier would produce by letting ThresholdOptimizer know what the sensitive features are\n",
    "    eo_model.fit(X, y, sensitive_features=sens)\n",
    "\n",
    "    return eo_model\n",
    "\n",
    "# Postprocess the trained classifier to satisfy Equalized Odds\n",
    "eoFair_clf= build_EOddFairModel(LR_clf,X_train, y_train, sens_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine EO-fair classifier on train data\n",
    "y_train_eo= eoFair_clf.predict(\n",
    "    X_train, sensitive_features=sens_train\n",
    ")\n",
    "# Examine EO-fair classifier on test data\n",
    "y_test_eo= eoFair_clf.predict(\n",
    "    X_test, sensitive_features=sens_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Print again report on test set for the previous models\n",
    "report_performance([uniform_clf, mode_clf, prior_clf,LR_clf],X_test,y_test,sens_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Print report for this EO-fair classifier\n",
    "\n",
    "print('-----------------')\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_test_eo))\n",
    "print(\"Precision:\", precision_score(y_test, y_test_eo))\n",
    "print(\"Recall:\", recall_score(y_test, y_test_eo))\n",
    "print(\"F1-score:\", f1_score(y_test, y_test_eo))\n",
    "print(\"Confusion Matrix:\", confusion_matrix(y_test, y_test_eo))\n",
    "print(\"Demographic Parity Difference:\", demographic_parity_difference(y_test, y_test_eo, sensitive_features=sens_test))\n",
    "print(\"Equalized Odds Difference:\", equalized_odds_difference(\n",
    "    y_test, y_test_eo, sensitive_features=sens_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices for the EO-fair and initial/naive classifiers \n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15,10))\n",
    "\n",
    "ConfusionMatrixDisplay(\n",
    "    confusion_matrix=confusion_matrix(y_test, LR_clf.predict(X_test)), \n",
    "    display_labels=[False, True]).plot(ax=ax[0],cmap=plt.cm.YlGnBu)\n",
    "ax[0].set_title(\"Naive LR\")\n",
    "\n",
    "ConfusionMatrixDisplay(\n",
    "    confusion_matrix=confusion_matrix(y_test, y_test_eo), \n",
    "    display_labels=[False, True],).plot(ax=ax[1],cmap=plt.cm.YlGnBu)\n",
    "ax[1].set_title(\"EO trained LR\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pick a dataset and upload it on [Aequitas](http://aequitas.dssg.io/) to generate a bias report!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

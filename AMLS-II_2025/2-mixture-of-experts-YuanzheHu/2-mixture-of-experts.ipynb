{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div width=50% style=\"display: block; margin: auto\">\n",
    "    <img src=\"figures/ucl-logo.svg\" width=100%>\n",
    "</div>\n",
    "\n",
    "### [UCL-ELEC0135 Applied Machine Learning Systems II - 2025]()\n",
    "University College London\n",
    "# Lab 2: Mixture of Experts\n",
    "\n",
    "\n",
    "<hr width=70% style=\"float: left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Mixture of experts (MoE) is a machine learning technique where multiple expert Neural Networks are used to divide a problem space into homogeneous regions. In this lab, we will use MoE in a classification task on [Cifar10](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
    "\n",
    "We will train a gating function to detect wether images are \"natural images\" (e.g. cat, dog, etc) or \"artificial images\" (e.g. plane, car). This gating functions will direct the samples to two deferent \"expert\" classifers, one trained to classify within the natural images catergory, and the other trained to classify within the artificial images catergory. Fianly, these experts will then used to boost the performance of a baseline 10 classes classiers.\n",
    "\n",
    "All models (gating function, experts, and baseline classifers) will be [Convolutional Neural Networks (CNNs)](https://en.wikipedia.org/wiki/Convolutional_neural_network), a variant of the Multi Layers Perceptron (MLP) seen in lab 1 that uses [Convolutional layers](https://en.wikipedia.org/wiki/Convolutional_layer) and [Pooling layers](https://en.wikipedia.org/wiki/Pooling_layer) to automate feature extraction.\n",
    "\n",
    "![](figures/moe_architecture_david.png)\n",
    "\n",
    "\n",
    "### Intended Learning Outcome\n",
    "* Define and train Convolutional Neural Networks with Tensorflow Keras.\n",
    "* Use Layers and Wrappers to define and combine custom gate models.\n",
    "* Compare the performances of a simple CNN classifier with a MoE classifier.\n",
    "\n",
    "\n",
    "### Outline\n",
    "\n",
    "This notebook has 4 parts:\n",
    "\n",
    "0. [Setting up](#0.-Setting-up)\n",
    "1. [Baseline 10 classes CNN](#1-baseline-10-classes-cnn)\n",
    "2. [Experts CNNs](#2-experts-cnns)\n",
    "3. [Integration of the models into a MoE](#3-gating-models)\n",
    "4. [MoE](#4-moe)\n",
    "\n",
    "<hr width=70% style=\"float: left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setting up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Importing librairies\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Make sure you are running this Notebook in the kernel of the virtual environment you created for this module.\n",
    "- Run the following cell. If some packages have not been installed, add them to the requirements.txt file, and run in terminal (after having activated the virtual environment you created) the command `pip install -r requirements.txt`. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-14 09:28:50.052081: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# TODO: run this cell and add packages as needed\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split # called scikit-learn when importing with pip (in requirements.txt)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Importing the data\n",
    "\n",
    "The [Cifar10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset consists of 60000 32x32 colour images in 10 classes {0: airplane, 1: automobile, 2: bird, 3: cat, 4: deer, 5: dog, 6: frog, 7: horse, 8: ship, 9: truck}, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
    "\n",
    "![](figures/cifar10_resize.png)\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Run the cell bellow to import the cifar0 dataset, select a subset of the training and test set, and normalize the data.\n",
    "- Print the number of samples in the training and test sets.\n",
    "- Display the distribution of labels, is this dataset balanced?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Since the cifar dataset is quite large, we are only going to select a subset of smaller size to reduce the computational load required to finish this lab.\n",
    "train_examples = 5000 # Max is 50000\n",
    "test_examples = 1000   # Max is 5000\n",
    "\n",
    "x_train = x_train[:train_examples] ; x_test = x_test[:test_examples]\n",
    "y_train = y_train[:train_examples] ; y_test = y_test[:test_examples]\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1 (helps models converge faster)\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Convert class vectors to binary class matrices (one-hot encoding)\n",
    "y_train0 = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test0 = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "print(\"y train0:{0}\\ny test0:{1}\".format(y_train0.shape, y_test0.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Baseline 10 classes CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline model and expert models will have the same CNN architecture, defined in the cell bellow. The large models should be more performent, but will require more training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN(n_classes = 10, large = False):\n",
    "    \"\"\"\n",
    "    Create a Convolutional Neural Network for image classification\n",
    "\n",
    "    Parameters:\n",
    "        - n_classes (int): number of classes in the dataset\n",
    "        - large (bool): whether to use a large or small model\n",
    "\n",
    "    Returns:\n",
    "        - model (tf.keras.models.Sequential): CNN model\n",
    "\n",
    "    \"\"\"\n",
    "    if large:\n",
    "        model = tf.keras.models.Sequential([\n",
    "            # Convolutions and Pooling layers for feature extraction\n",
    "            tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu', name='conv1.1', input_shape=(32, 32, 3)),\n",
    "            tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu', name='conv1.2'),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2), name='pool1'),\n",
    "            tf.keras.layers.Dropout(0.25, name='drop1'),\n",
    "            tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu', name='conv2.1'),\n",
    "            tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu', name='conv2.2'),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2), name='pool2'),\n",
    "            tf.keras.layers.Dropout(0.25, name='drop2'),\n",
    "            tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu', name='conv3.1'),\n",
    "            tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu', name='conv3.2'),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2), name='pool3'),\n",
    "            tf.keras.layers.Dropout(0.25, name='drop3'),\n",
    "\n",
    "            # MLP for classification\n",
    "            tf.keras.layers.Flatten(name='flatten'),\n",
    "            tf.keras.layers.Dense(512, activation='relu', name='dense1'),\n",
    "            tf.keras.layers.Dropout(0.5, name='drop4'),\n",
    "            tf.keras.layers.Dense(n_classes, activation='softmax', name='output')\n",
    "        ])\n",
    "\n",
    "    else:\n",
    "        model = tf.keras.models.Sequential([\n",
    "            # Convolutions and Pooling layers for feature extraction\n",
    "            tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu', name='conv1.1', input_shape=(32, 32, 3)),\n",
    "            tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu', name='conv1.2'),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2), name='pool1'),\n",
    "            tf.keras.layers.Dropout(0.25, name='drop1'),\n",
    "            tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu', name='conv2.1'),\n",
    "            tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu', name='conv2.2'),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2), name='pool2'),\n",
    "            tf.keras.layers.Dropout(0.25, name='drop2'),\n",
    "\n",
    "            # MLP for classification\n",
    "            tf.keras.layers.Flatten(name='flatten'),\n",
    "            tf.keras.layers.Dense(512, activation='relu', name='dense1'),\n",
    "            tf.keras.layers.Dropout(0.5, name='drop3'),\n",
    "            tf.keras.layers.Dense(n_classes, activation='softmax', name='output')\n",
    "        ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Use the function above to create a baseline CNN.\n",
    "- Compile the model, use the [ADAM optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) with a learning rate of 0.00001, and the [categorical crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy) loss funtion.\n",
    "- Use model.summary() to print a summary of your architecture.\n",
    "- Train the model and display the confusion matrix on the test set. You can use the function [`confusion_matrix`](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.confusion_matrix.html) from sklearn, and [`sns.heatmap`](https://seaborn.pydata.org/generated/seaborn.heatmap.html).\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "<b>üí° Tips</b> \n",
    "\n",
    "- Go back to lab 1 for a refresher on how to compile and train a model with tensorflow keras.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: your code here\n",
    "\n",
    "baseline_model = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Experts CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Natural Images Expert CNN\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Use the function `CNN` defined in task 1 to create the Natural Images Expert.\n",
    "- Compile the model, use the [ADAM optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) with a learning rate of 0.00001, and the [categorical crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy) loss funtion.\n",
    "- Use model.summary() to print a summary of your architecture.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: your code here\n",
    "\n",
    "natural_expert_model = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Create a subset of the training data containing all samples from the \"natural\" classes (labels 2, 3, 4, 5, 6 and 7).\n",
    "- Train the expert on the subset.\n",
    "- Display 2 confusion matrices, one with the prediction results the test set when only including the \"natural\" classes, and one on the entire test set.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Artificial Images Expert CNN\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Use the function `CNN` defined in task 1 to create the Artificial Images Expert.\n",
    "- Compile the model, use the [ADAM optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) with a learning rate of 0.00001, and the [categorical crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy) loss funtion.\n",
    "- Use model.summary() to print a summary of your architecture.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: your code here\n",
    "\n",
    "artificial_expert_model = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Create a subset of the training data containing all samples from the \"artificial\" classes labels 0, 1, 8 and 9.\n",
    "- Train the expert on the subset.\n",
    "- Display 2 confusion matrices, one with the prediction results the test set when only including the \"natural\" classes, and one on the entire test set.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gating models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Gating Natural / Artificial images Experts\n",
    "\n",
    "The first gating model will take as input an image from the cifar dataset, and decide which expert to use. In other words, it's a binairy classifier trained to know if an image falls into the \"natural\" or \"artificial\" category.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- For the training and test set, create new labels that indicate wether the samples are \"natural\" objects or \"artificial\" objects. Use one-hot encoding (see code in task 0.2 for reference).\n",
    "- Use the function `CNN` defined in task 1 to create a gating binary classfifier.\n",
    "- Use model.summary() to print a summary of your architecture.\n",
    "- Train the model and display the confusion matrix on the test set.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: your code here\n",
    "\n",
    "expert_choice_gate = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Gating Baseline / Experts\n",
    "\n",
    "We now have a baseline classifier (10 classes CNN), 2 experts classifiers (10 classes but specialise in classifying either the natural or artificial categories), and a gating classfier (2 classes CNN) which decides which of the experts to choose for the given input. \n",
    "\n",
    "We now want our second gate to be able to take in the cifar input and weighs the importance of the output of the baseline and the selected expert in producing a final MoE output prediction. To do this, the second gate will be composed of 2 sub-gates, one for each expert model. \n",
    "\n",
    "Each gate will use an importance model that will output values for the i) baseline and ii) chosen expert, i.e. what the sub-gate 'thinks' the realtive importance of the 10 baseline classifier and the relative importance is of the expert are.\n",
    "\n",
    "**Importance model:**\n",
    "1. Flatten Layer \n",
    "2. Dense layer with 512 neurons and relu activation function\n",
    "3. Dropout layer with dropout rate of 0.5\n",
    "4. Dense layer with 2*the number of classes and softmax activation functions\n",
    "5. Reshape layer that reshapes the output to (number of classes, 2)\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Implement the function `importance_model` to create a model with the arcitecture described above.\n",
    "- Create 2 sub gate models, one for the \"natural\" expert and one for the \"artificial\" expert.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importance_model(n_classes):\n",
    "    # TODO: your code here\n",
    "    pass\n",
    "\n",
    "# TODO: your code here\n",
    "artificial_importance_model = ...\n",
    "natural_importance_model = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can implement the subgate as a model that:\n",
    "\n",
    "- Takes in argument gx, which is a list of 3 tensors: \n",
    "    - gx[0] -> baseline output tensor (10,). Is a softmax output for each of the 10 classes\n",
    "    - gx[1] -> expert network output tensor (10,). Is a softmax output for each of the 10 classes. Which expert's output reached here depends on the binary classifier output of the first gate (we will implement the logic of choosing the expert when we integrate all the models together, see below).\n",
    "    - the corresponding importance of each models\n",
    "    - gx[2] -> corresponding importance model output tensor (10,2,). \n",
    "        - gx[2][:,:,0] -> baseline importance tensor of shape (10,) i.e what the sub-gate thinks the importance is of each of the 10 baseline output classes. \n",
    "        - gx[2][:,:,1] -> expert importance tensor of shape (10,) i.e what the sub-gate thinks the importance is of each of the 10 expert output classes.\n",
    "\n",
    "- Performs the following:\n",
    "    - Multiplies the baseline's output by the sub-gate's baseline importance -> (10,) tensor.\n",
    "    - Multiplies the expert's output by the sub-gate's expert importance -> (10,) tensor \n",
    "    - Sums these two importance-weighted terms to get a final (10,) tensor of logit outputs (one for each class) -> (10,) tensor.\n",
    "\n",
    "To implement the subgate, we use a [`tf.keras.layers.Lamdba`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Lambda) layer, itself using a [lambda](https://www.w3schools.com/python/python_lambda.asp) object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgate(baseline_model, expert_model, importance_model, n_classes=10):\n",
    "    output = tf.keras.layers.Lambda(\n",
    "        lambda gx: (gx[0]*gx[2][:,:,0]) + (gx[1]*gx[2][:,:,1]), output_shape=(n_classes,)\n",
    "        )([baseline_model, expert_model, importance_model])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. MoE\n",
    "\n",
    "At this stage, we have implemented:\n",
    "- A baseline 10 classes classifier.\n",
    "- 2 experts 10 classes classifiers.\n",
    "- A binairy classifier that will be used to choose which expert to use.\n",
    "- Subgates that weighs the importance of the output of the baseline and the selected expert in producing a final MoE output prediction using an importance model.\n",
    "\n",
    "We now need to implement the logic deciding which expert to use, and integrate all the different components of our MoE together in order to be able to train the whole architecture at once.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "<b>üí° Tips</b> \n",
    "\n",
    "- The advantage of using a framework like Tensorflow Keras is that all the layers are inheritted from the same abstract classes, meaning that as long as we use classes and wrappers properly, and that input and output shapes of the components models match, the resulting model will be trainable using the same tools as the individual component models. This spares us extensive and complicated gradiant calculations.\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Using a [`tf.keras.layers.Lamdba`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Lambda) layer and [`tf.where`](https://www.tensorflow.org/api_docs/python/tf/where) in a similar manner as the implementation of the function `subgate`, implement a model that:\n",
    "    - Takes in argument gx, which is a list of 6 tensors:\n",
    "        - gx[0] -> baseline output tensor (10,)\n",
    "        - gx[1] -> first gate binary output tensor (2,)\n",
    "        - gx[2] -> artificial expert output tensor (10,)\n",
    "        - gx[3] -> natural expert output tensor (10,)\n",
    "        - gx[4] -> artificial importance model output tensor (10,2,)\n",
    "        - gx[5] -> natural importance model output tensor (10,2,)\n",
    "    - If the binairy classifier classifies the image as artificial (`switch(tf.expand_dims(gx[1][:,0],axis=1) > tf.expand_dims(gx[1][:,1],axis=1)`):\n",
    "        - Outputs the results of the artificial subgate model (10,2,)\n",
    "    - Else:\n",
    "        - Outputs the results of the natural subgate model (10,2,)\n",
    "\n",
    "- Compile the model, use the [ADAM optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) with a learning rate of 0.00001, and the [categorical crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy) loss funtion.\n",
    "- Use model.summary() to print a summary of your architecture.\n",
    "- Train the model and display the confusion matrix on the test set. Compare performances with the baseline classifier alone.\n",
    "    - Have the performances been improved?\n",
    "    - Is the increased model complexity and training time worth the accuracy gains?\n",
    "\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MoE(baseline_model, expert_choice_gate, artificial_expert_model, natural_expert_model, artificial_importance_model, natural_importance_model):\n",
    "    \"\"\"\n",
    "    Create a Mixture of Experts model for image classification\n",
    "\n",
    "    Args:\n",
    "        - baseline_model (tf.keras.models.Sequential): baseline CNN model\n",
    "        - expert_choice_gate (tf.keras.models.Sequential): expert choice gate model\n",
    "        - artificial_expert_model (tf.keras.models.Sequential): artificial expert model\n",
    "        - natural_expert_model (tf.keras.models.Sequential): natural expert model\n",
    "        - artificial_importance_model (tf.keras.models.Sequential): artificial importance model\n",
    "        - natural_importance_model (tf.keras.models.Sequential): natural importance model\n",
    "\n",
    "    Returns:\n",
    "        - model (tf.keras.models.Model): MoE model\n",
    "\n",
    "    \"\"\"\n",
    "    # Define input tensor\n",
    "    inputs = tf.keras.layers.Input(shape=(32, 32, 3))\n",
    "\n",
    "    # Get baseline predictions\n",
    "    baseline_output = baseline_model(inputs)\n",
    "    \n",
    "    # Get expert choice gate predictions\n",
    "    expert_gate_output = expert_choice_gate(inputs)\n",
    "\n",
    "    # Get expert predictions\n",
    "    artificial_expert_output = artificial_expert_model(inputs)\n",
    "    natural_expert_output = natural_expert_model(inputs)\n",
    "\n",
    "    # Get importance model outputs\n",
    "    artificial_importance_output = artificial_importance_model(inputs)\n",
    "    natural_importance_output = natural_importance_model(inputs)\n",
    "\n",
    "    # Condition selection: If expert_choice_gate prefers artificial expert, use it, else use natural expert\n",
    "    def moe_logic(gx):\n",
    "        \"\"\"\n",
    "        Mixture of Experts logic, selecting the expert output based on the expert choice gate\n",
    "        \n",
    "        Args:\n",
    "            - gx (list): list of tensors containing expert_choice, baseline, artificial_expert, natural_expert, artificial_importance, natural_importance\n",
    "            \n",
    "        Returns:\n",
    "            - selected_expert_output (tf.Tensor): selected expert output\n",
    "        \"\"\"\n",
    "        expert_choice, base_out, art_exp_out, nat_exp_out, art_imp_out, nat_imp_out = gx\n",
    "        condition = tf.expand_dims(expert_choice[:, 0], axis=1) > tf.expand_dims(expert_choice[:, 1], axis=1)\n",
    "        \n",
    "        # Select the expert output based on the expert choice gate using tf.where\n",
    "        # TODO: your code here\n",
    "        selected_expert_output = ...\n",
    "\n",
    "        return selected_expert_output\n",
    "\n",
    "    # MoE decision layer using tf.keras.layers.Lambda\n",
    "    # TODO: your code here\n",
    "    output = ...\n",
    "\n",
    "    # Build the final model\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "    return model\n",
    "\n",
    "# TODO: your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amls2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

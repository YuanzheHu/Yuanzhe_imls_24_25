{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div width=50% style=\"display: block; margin: auto\">\n",
    "    <img src=\"figures/ucl-logo.svg\" width=100%>\n",
    "</div>\n",
    "\n",
    "### [UCL-ELEC0135 Advanced Machine Learning Systems II - 2025]()\n",
    "University College London\n",
    "# Lab 3: Reinforcement Learning\n",
    "\n",
    "\n",
    "<hr width=70% style=\"float: left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by interacting with its environment. Through trial and error, the agent observes the outcomes of its actions and gradually learns to take optimal actions to maximize long-term rewards. RL is widely used in areas such as game AI, robotics, and recommendation systems, making it a fundamental tool for building intelligent systems.\n",
    "\n",
    "In this lab, you will use **Gymnasium**, a widely used RL library, to train different types of models to play **Blackjack**, a classic card game of decision-making under uncertainty. You will explore various RL approaches, from **tabular Q-learning** to **Deep Q Networks (DQN) with Experience Replay**, and observe how different training strategies affect the agent's performance. Through Gymnasium‚Äôs Blackjack environment, you will gain hands-on experience in defining states, actions, and rewards in a real-world game setting.\n",
    "\n",
    "By the end of this lab, you will have a solid understanding of reinforcement learning principles and practical experience in training RL agents using **Gymnasium**. You will also compare different RL models and analyze their effectiveness in learning Blackjack strategies.\n",
    "\n",
    "### Intended Learning Outcome\n",
    "* Understand what the core components of a Gymnasium Reinforcement Learning environment.\n",
    "* Implement TD Learning with an epsilon greedy policy on a tabular agent.\n",
    "* Implement TD Learning with an epsilon greedy policy on a Deep Q-Network (DQN) agent.\n",
    "* Realise the importance of vectorisation for efficient learning.\n",
    "* Implement experience replay to improve the DQN agent.\n",
    "* Implement double-DQN to improve the DQN agent.\n",
    "\n",
    "### Outline\n",
    "\n",
    "This notebook has 3 parts:\n",
    "\n",
    "0. [Setting up](#0.-Setting-up)\n",
    "1. [Defining and Training a tabular Q-Learning agent](#1-defining-and-training-a-tabular-q-learning-agent)\n",
    "2. [Defining and Training a Deep Q-Network agent](#2-defining-and-training-a-deep-q-network-agent)\n",
    "3. [Improving the DQN agent performences](#3-improving-the-dqn-agents-performences)\n",
    "\n",
    "<hr width=70% style=\"float: left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setting up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Importing librairies\n",
    "\n",
    "### Gymnasium\n",
    "\n",
    "![](figures/gymnasium-text.png)\n",
    "\n",
    "[Gymnasium](https://gymnasium.farama.org) is a Python library for developing and evaluating reinforcement learning (RL) algorithms maintained by the nonprofit [Farama](https://farama.org). It provides a standardized API for RL environments, allowing researchers and developers to easily test and compare RL agents across different tasks. \n",
    "\n",
    "Gymnasium is a maintained fork of [OpenAI Gym](https://www.gymlibrary.dev/index.html), created after OpenAI stopped actively supporting Gym. It builds on Gym‚Äôs core functionality while introducing bug fixes, enhanced support for modern RL libraries, and better long-term maintenance. Many RL frameworks, such as Stable-Baselines3 and RLlib, now support Gymnasium as a direct replacement for Gym. \n",
    "\n",
    "Gymnasium contains in its source code multiple canonical environments meant to serve as tutorial examples and research benchmarks. It also provides documentation on how to create custom third party environments.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Make sure you are running this Notebook in the kernel of the virtual environment you created for this module.\n",
    "- Run the following cell. If some packages have not been installed, add them to the requirements.txt file, and run in terminal (after having activated the virtual environment you created) the command `pip install -r requirements.txt`. \n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "<b>üí° Tips</b> \n",
    "\n",
    "- Make sure to specify `numpy<2` in the `requirements.txt` file.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions and classes for the utils.py file\n",
    "from utils import evaluate_blackjack_agent # Function to evaluate the agent in the Blackjack environment on a number of episodes\n",
    "from utils import unpack_qvalues_dict # Function to unpack the Q-values dictionary into a matrix\n",
    "from utils import get_best_actions_from_DQN # Function to get the Q-values from the DQN model\n",
    "from utils import ExperienceReplayBuffer # Class for the experience replay buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Setting up the RL environment and evaluating benchmark win rates values\n",
    "\n",
    "In this lab, we will train an agent to play the game Blackjack. We must first setup an environment, i.e the external system or simulation with which the agent interacts, providing states, receiving actions, and returning rewards and new states based on those actions.\n",
    "\n",
    "### The Gymnasium BlackJack Environment\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Read the following, and make sure you understand clearly the rules of the game we are training a RL agent for.\n",
    "\n",
    "</div>\n",
    "\n",
    "[Gymnasium's Blackjack](https://gymnasium.farama.org/environments/toy_text/blackjack/#blackjack_ref) environment implements the Blackjack game discribed in [Reinforcement learning : an introduction / Richard S. Sutton and Andrew G. Barto.](http://www.incompleteideas.net/book/RLbook2020.pdf) example 5.1 on page 93 (giving you a free textbook, you're welcome).\n",
    "\n",
    "![](figures/blackjack.gif)\n",
    "\n",
    "1. **Observation Space**:\n",
    "  - The **observation space** represents the state of the environment visible to the agent.\n",
    "  - In the game Blackjack, the observation consists of a 3-tuple containing: the player‚Äôs current sum (between 0 and 32), the value of the dealer‚Äôs one showing card (1-10 where 1 is ace), and whether the player holds a usable ace (0 or 1)\n",
    "  - `obs: (int, int, bool)`\n",
    "\n",
    "2. **Action Space**:\n",
    "  - The **action space** defines the possible moves the agent can take.\n",
    "  - In the game Blackjack, the action concists of a signle value in the range {0, 1} indicating whether to stick (stop playing) or hit (ask for another card).\n",
    "  - `action: bool`\n",
    "\n",
    "3. **Rewards**:\n",
    "  - The agent receives a reward based on its actions:\n",
    "    - win game: +1\n",
    "    - lose game: -1\n",
    "    - draw game: 0\n",
    "    - win game with natural blackjack: +1.5 (if natural is True) +1 (if natural is False)\n",
    "  - `reward: float`\n",
    "\n",
    "4. **Dynamics**:\n",
    "  - The environment updates based on the agent's actions and defined transition rules.\n",
    "  - The episode ends if the following happens:\n",
    "    - The player hits and the sum of hand exceeds 21.\n",
    "    - The player sticks.\n",
    "    - An ace will always be counted as usable (11) unless it busts the player.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark win rates\n",
    "\n",
    "To assess our agents' performances, let's create two policies manually that will serve as benchmarcks.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Implement the function `random_policy` in the cell below to randomly select an action to play at each turn, independently of the observation.\n",
    "- Implement the function `basic_strategy`in the cell below to follow the policy:\n",
    "    - If the player sum is stricktly bellow 12, hit (action 1).\n",
    "    - If the player sum is stricktly above 17, stick (action 0).\n",
    "    - If the player sum is between 16 and 16, and that the dealer has a card above 7, hit (action 1).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Random Policy\n",
    "def random_policy(obs, env):\n",
    "    \"\"\"\n",
    "    A random policy that samples actions uniformly at random.\n",
    "\n",
    "    Args:\n",
    "        obs: The current observation.\n",
    "    \"\"\"\n",
    "    # Return a random action\n",
    "    # TODO: Implement the random policy\n",
    "    return ...\n",
    "\n",
    "# Define the Basic Strategy Agent\n",
    "def basic_strategy(obs,env):\n",
    "    \"\"\"\n",
    "    Implements a simple Blackjack basic strategy.\n",
    "    - Hit if player's sum is < 12.\n",
    "    - Stick if sum is >= 17.\n",
    "    - For 12-16, hit if the dealer has a high card (7 or higher).\n",
    "    \"\"\"\n",
    "    # Unpack the observation\n",
    "    player_sum, dealer_card, usable_ace = obs\n",
    "\n",
    "    # Implement the basic strategy\n",
    "    # TODO: your code here\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Run the cells bellow to evaluate the policies and visualise the results.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluation function\n",
    "def evaluate_blackjack_policy(policy, env, num_episodes=10000):\n",
    "    \"\"\"\n",
    "    Evaluates the given agent in the Blackjack environment.\n",
    "\n",
    "    Args:\n",
    "        agent: The agent (policy) to evaluate (can be a random agent or a basic strategy agent).\n",
    "        env: The Blackjack environment.\n",
    "        num_episodes: The number of episodes to run for evaluation.\n",
    "\n",
    "    Returns:\n",
    "        win_rate: The fraction of games won by the agent.\n",
    "        draw_rate: The fraction of games that resulted in a draw.\n",
    "        loss_rate: The fraction of games lost by the agent.\n",
    "    \"\"\"\n",
    "    wins, draws, losses = 0, 0, 0\n",
    "\n",
    "    for _ in tqdm(range(num_episodes)):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy(obs,env)  # Get the action from the agent\n",
    "            obs, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        if reward > 0:\n",
    "            wins += 1\n",
    "        elif reward == 0:\n",
    "            draws += 1\n",
    "        else:\n",
    "            losses += 1\n",
    "\n",
    "    win_rate = wins / num_episodes\n",
    "    draw_rate = draws / num_episodes\n",
    "    loss_rate = losses / num_episodes\n",
    "\n",
    "    return win_rate, draw_rate, loss_rate\n",
    "\n",
    "# Initialize the environment\n",
    "n_episodes = 100000\n",
    "# Use a wrapper to record the episode statistics\n",
    "env = gym.make('Blackjack-v1', natural=False, sab=False)\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=n_episodes) \n",
    "\n",
    "# Evaluate the Random Policy\n",
    "win_rate_random, draw_rate_random, loss_rate_random = evaluate_blackjack_policy(random_policy, env, 100000)\n",
    "\n",
    "# Evaluate the Basic Strategy\n",
    "win_rate_basic, draw_rate_basic, loss_rate_basic = evaluate_blackjack_policy(basic_strategy, env, 100000)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(# Return the results\n",
    "{\n",
    "    \"Random Policy\": {\n",
    "        \"Win Rate\": win_rate_random,\n",
    "        \"Draw Rate\": draw_rate_random,\n",
    "        \"Loss Rate\": loss_rate_random\n",
    "    },\n",
    "    \"Basic Strategy\": {\n",
    "        \"Win Rate\": win_rate_basic,\n",
    "        \"Draw Rate\": draw_rate_basic,\n",
    "        \"Loss Rate\": loss_rate_basic\n",
    "    }\n",
    "})\n",
    "\n",
    "# Plot the action taken by the random policy\n",
    "random_policy_actions = np.zeros((32, 11, 2))\n",
    "for player_sum in range(32):\n",
    "    for dealer_card in range(1, 12):\n",
    "        for ace in [0, 1]:\n",
    "            obs = (player_sum, dealer_card, ace)\n",
    "            random_policy_actions[player_sum, dealer_card - 1, ace] = random_policy(obs, env)\n",
    "\n",
    "# Plot the action taken by the basic strategy\n",
    "basic_strategy_actions = np.zeros((32, 11, 2))\n",
    "for player_sum in range(32):\n",
    "    for dealer_card in range(1, 12):\n",
    "        for ace in [0, 1]:\n",
    "            obs = (player_sum, dealer_card, ace)\n",
    "            basic_strategy_actions[player_sum, dealer_card - 1, ace] = basic_strategy(obs, env)\n",
    "\n",
    "# Plot the actions\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "for i, usable_ace in enumerate([False, True]):\n",
    "    im = axs[0, i].imshow(random_policy_actions[:, :, int(usable_ace)], aspect=\"auto\")\n",
    "    axs[0, i].set_title(f\"Random Policy with usable ace = {usable_ace}\")\n",
    "    axs[0, i].set_xlabel(\"Dealer's face-up card\")\n",
    "    axs[0, i].set_ylabel(\"Player's sum\")\n",
    "    fig.colorbar(im, ax=axs[0, i])\n",
    "\n",
    "    im = axs[1, i].imshow(basic_strategy_actions[:, :, int(usable_ace)], aspect=\"auto\")\n",
    "    axs[1, i].set_title(f\"Basic Strategy with usable ace = {usable_ace}\")\n",
    "    axs[1, i].set_xlabel(\"Dealer's face-up card\")\n",
    "    axs[1, i].set_ylabel(\"Player's sum\")\n",
    "    fig.colorbar(im, ax=axs[1, i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The goal of this lab is to train models that learn a policy maximizing the win rate. At a minimum, an agent should improve its performance and achieve a win rate higher than that of a Random policy.\n",
    "\n",
    "**In the case of Blackjack, the game is simple enough that finding a successful policy is relatively easy‚Äîyou might even discover a strategy that outperforms some of the agents you train in this lab. However, many real-world tasks are far too complex to allow for an obvious winning strategy. This is where reinforcement learning (RL) truly shines. By using the tools introduced in this lab, you will be able to tackle much more challenging problems.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Defining and Training a tabular Q-Learning agent\n",
    "*Task 1 is based on the [Training an agent tutorial](https://gymnasium.farama.org/introduction/train_agent/) in Gymnasium's documentation.*\n",
    "\n",
    "In Task 1, we will implement an tabular Q-learning agent that will use **TD-learning** and a **eplison greedy policy** to learn how to play Blackjack.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "<b>üí° Tips</b> \n",
    "\n",
    "- Review the following elements of theory, you will need them later.\n",
    "\n",
    "</div>\n",
    "\n",
    "### Shape of the Q(s, a) Object in Tabular Q-Learning\n",
    "\n",
    "In **Tabular Q-Learning**, the **Q-table** is a **2D array (matrix)** where:\n",
    "\n",
    "$$\n",
    "Q \\in \\mathbb{R}^{|\\mathcal{S}| \\times |\\mathcal{A}|}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ |\\mathcal{S}| $ is the number of discrete states.\n",
    "- $ |\\mathcal{A}| $ is the number of discrete actions.\n",
    "- Each entry $ Q(s, a) $ represents the estimated **Q-value** for taking action $ a $ in state $ s $.\n",
    "\n",
    "For example, if an environment has **3 states** and **2 actions**, then at time $t$, $Q(s_t,a_t)$ is:\n",
    "\n",
    "| State | Action 0 | Action 1 | \n",
    "| ----- | -------- | -------- | \n",
    "| s0    |Q(s0_t,a0_t)| Q(s0_t,a1_t)|\n",
    "| s1    |Q(sA_t,a0_t)|Q(s1_t,a1_t)|\n",
    "| s2    |Q(s2_t,a0_t)|Q(s2_t,a1_t)|\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Eplison greedy policy\n",
    "\n",
    "The **epsilon-greedy policy** is a strategy for balancing **exploration** and **exploitation** in **Reinforcement Learning (RL)**.\n",
    "\n",
    "**Definition** \n",
    "\n",
    "The **epsilon-greedy** policy selects the action with the highest estimated value most of the time (**exploitation**) but occasionally chooses a random action (**exploration**) with a probability of **Œµ (epsilon)**.\n",
    "\n",
    "At each time step $t$, the action $a_t$ taken based on the observation $s_t$ is chosen as:\n",
    "\n",
    "\n",
    "$$\n",
    "a_t =\n",
    "\\begin{cases}\n",
    "\\arg\\max_{a_t} Q(s_t,a_t), & \\text{with probability } 1 - \\epsilon \\quad (\\text{greedy choice}) \\\\\n",
    "\\text{random action}, & \\text{with probability } \\epsilon \\quad (\\text{exploration})\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $Q(s_t,a_t)$ is the estimated q-value of action $a_t$.\n",
    "- $\\epsilon$ (epsilon) is a small value (e.g., 0.1) that controls the trade-off between exploration and exploitation.\n",
    "\n",
    "To transition the agent from an exploratory phase to a more exploitative one over time, we use the **epsilon decay** technique by reducing the value of epsilon over time.\n",
    "\n",
    "---\n",
    "\n",
    "### TD-learning\n",
    "\n",
    "**Temporal Difference (TD) Learning** is a fundamental method in **Reinforcement Learning (RL)** which allows agents to learn directly from experience without requiring a complete model of the environment.\n",
    "\n",
    "**Definition**\n",
    "\n",
    "TD learning updates value estimates using the difference (or \"error\") between successive estimates. This is called the **TD error**, and is defined as:\n",
    "\n",
    "$$\n",
    "Q(s_{t+1},a_{t+1}) \\leftarrow Q(s_t,a_t) + \\alpha \\cdot (r + \\gamma \\cdot Q(s_t',a_t') - Q(s_t,a_t))\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ Q(s_{t+1},a_{t+1})$ is the value estimate for state $ s $ according to the next policy.\n",
    "- $ Q(s_t,a_t) $ is the value estimate for state $ s $ according to the current policy.\n",
    "- $ \\alpha $ is the **learning rate** (step size).\n",
    "- $ r $ is the **reward** received after taking an action in state $ s $.\n",
    "- $ \\gamma $ is the **discount factor** (determines the importance of future rewards).\n",
    "- $ Q(s_t',a_t') $ is the estimate of the q-values for the next state $ s' $ according to the current policy.\n",
    "- $ \\delta = r + \\gamma Q(s_t',a_t') - Q(s_t,a_t) $ is the **TD error**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Implementing the agent\n",
    "\n",
    "The class `TabularQLearningAgent` contains the code needed to define an agent that we will train to learn how to play the Blackjack game.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Implement the function `get_action` in the class `TabularQLearningAgent` with the eplison greedy policy.\n",
    "- Implement the function `update` in the class `TabularQLearningAgent` that uses TD Learning to update the `TabularQLearningAgent.q_values` object after a game.\n",
    "- Implement the function `decay_epsilon` in the class `TabularQLearningAgent` that updates the agent's epsilon parameters until it reaches its final value.\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "<b>üí° Tips</b> \n",
    "\n",
    "- In the case of our game of Blackjack, there are `2` actions, and `32 * 11 * 2 = 704` states. The Q table will therefore be 704x2 table will all values initialised to 0, and updated during training.\n",
    "- The `TabularQLearningAgent.q_values` object is a `defaultdict`. This is a special class of key<->pair dictionary that is initialised with a default output. When called with a key , if this key is not recognised, it returns the default output. This object allows for a flexible implementation of the Q table, where only the state<->action pairs that have been updated are stored.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularQLearningAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        learning_rate: float,\n",
    "        initial_epsilon: float,\n",
    "        epsilon_decay: float,\n",
    "        final_epsilon: float,\n",
    "        discount_factor: float = 0.95,\n",
    "    ):\n",
    "        \"\"\"Initialize a Reinforcement Learning agent with an empty dictionary\n",
    "        of state-action values (q_values), a learning rate and an epsilon.\n",
    "\n",
    "        Args:\n",
    "            env: The training environment\n",
    "            learning_rate: The learning rate\n",
    "            initial_epsilon: The initial epsilon value\n",
    "            epsilon_decay: The decay increment for the epsilon value\n",
    "            final_epsilon: The final epsilon value \n",
    "            discount_factor: The discount factor for computing the Q-value\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.q_values = defaultdict(lambda: np.zeros(env.action_space.n)) # Q-values for each state-action pair, it is of the same shape as the action space\n",
    "\n",
    "        self.lr = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        self.epsilon = initial_epsilon # epsilon value of the agent\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "\n",
    "        self.training_error = [] # temporal difference error for each update\n",
    "\n",
    "    def get_action(self, obs: tuple[int, int, bool]) -> int:\n",
    "        \"\"\"\n",
    "        Returns the action to take given the observation using an epsilon-greedy policy.\n",
    "\n",
    "        Args:\n",
    "            obs(tuple): The observation of the environment\n",
    "                obs[0](int): The player's current sum\n",
    "                obs[1](int): The dealer's face-up card\n",
    "                obs[2](bool): Whether the player has a usable ace\n",
    "\n",
    "        Returns:\n",
    "            The action to take (0 for stick, 1 for hit)\n",
    "        \"\"\"\n",
    "        # TODO: your code here\n",
    "        ...\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        obs: tuple[int, int, bool],\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        terminated: bool,\n",
    "        next_obs: tuple[int, int, bool],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Updates the Q-value of the state-action pair using TD-learning.\n",
    "\n",
    "        Args:\n",
    "            obs(tuple): The observation of the environment\n",
    "                obs[0](int): The player's current sum\n",
    "                obs[1](int): The dealer's face-up card\n",
    "                obs[2](bool): Whether the player has a usable ace\n",
    "            action(int): The action taken\n",
    "            reward(float): The reward received\n",
    "            terminated(bool): Whether the episode has terminated\n",
    "            next_obs(tuple): The next observation of the environment\n",
    "                next_obs[0](int): The player's current sum\n",
    "                next_obs[1](int): The dealer's face-up card\n",
    "                next_obs[2](bool): Whether the player has a usable\n",
    "\n",
    "        Updated parameters:\n",
    "            self.q_values: The Q-values for each state-action pair\n",
    "            self.training_error: The temporal difference error for each update\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        # Compute the future Q-value of the next observation\n",
    "        # if the episode has terminated, the future Q-value is 0, otherwise it is the maximum Q-value of the next observation\n",
    "        # TODO: your code here\n",
    "        future_q_value = ... # if the episode has terminated, the future Q-value is 0, otherwise it is the maximum Q-value of the next observation\n",
    "\n",
    "        # TODO: your code here\n",
    "        # Compute the temporal difference error according to the TD-learning formula which includes the reward, the future Q-value and the current Q-value for the state-action pair\n",
    "        temporal_difference = ...\n",
    "\n",
    "\n",
    "        # Update the Q-value of the state-action pair using the TD-learning formula\n",
    "        # TODO: your code here\n",
    "        self.q_values[obs][action] = ...\n",
    "\n",
    "        self.training_error.append(temporal_difference)\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"\n",
    "        Decays the epsilon value.\n",
    "        \n",
    "        The epsilon value is decayed by the decay increment\n",
    "        and is clipped to the final epsilon value.\n",
    "\n",
    "        Updated parameters:\n",
    "            self.epsilon: The epsilon value of the agent\n",
    "        \"\"\"\n",
    "        # TODO: your code here\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Training the agent\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Implement the function `train_blackjack_agent` bellow. Each comment `# TODO` indicates that some code must be implemented.\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "<b>üí° Tips</b> \n",
    "\n",
    "- Run `help(gym.envs.toy_text.blackjack.BlackjackEnv.step)` to print the method's doctring.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_blackjack_tabular_agent(agent, env:gym.envs.toy_text.blackjack.BlackjackEnv, n_episodes:int)->None:\n",
    "    \"\"\"\n",
    "    Trains the agent using the Q-learning algorithm.\n",
    "\n",
    "    Args:\n",
    "        agent: The Q-learning agent\n",
    "        env: The training environment\n",
    "        n_episodes(int): The number of episodes to train the agent\n",
    "\n",
    "    \"\"\"\n",
    "    for episode in tqdm(range(n_episodes)):\n",
    "        obs, info = env.reset() # reset the environment and get the initial observation\n",
    "        done = False\n",
    "\n",
    "        # play one episode\n",
    "        while not done:\n",
    "            # get the action from the agent\n",
    "            # TODO: your code here\n",
    "            action = ...\n",
    "\n",
    "            # take a step in the environment using the action with BlackjackEnv.step and get the next observation, reward, termination, and truncation\n",
    "            # TODO: your code here\n",
    "            next_obs, reward, terminated, truncated, info = ...\n",
    "\n",
    "            # update the agent with the observation, action, reward, termination, and next observation\n",
    "            # TODO: your code here\n",
    "            ...\n",
    "\n",
    "            # update if the environment is done (terminated or truncated is True)\n",
    "            # TODO: your code here\n",
    "            done = ...\n",
    "\n",
    "            # update the observation to the next observation\n",
    "            # TODO: your code here\n",
    "            obs = ...\n",
    "\n",
    "        # update the agent's epsilon value\n",
    "        # TODO: your code here\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Run the cells bellow to train the agent and visualise the results.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 100000\n",
    "env = gym.make('Blackjack-v1', natural=False, sab=False)\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=n_episodes)\n",
    "\n",
    "# Set hyperparameters\n",
    "learning_rate = 0.01\n",
    "start_epsilon = 1.0\n",
    "epsilon_decay = start_epsilon / (n_episodes / 2)  # reduce the exploration over time\n",
    "final_epsilon = 0.1\n",
    "\n",
    "\n",
    "# Create the agent\n",
    "agent_tabular = TabularQLearningAgent(\n",
    "    env=env,\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=start_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    final_epsilon=final_epsilon,\n",
    ")\n",
    "\n",
    "# Train the agent\n",
    "train_blackjack_tabular_agent(agent_tabular, env, n_episodes)\n",
    "\n",
    "# Print the win rate of the agent\n",
    "win_rate_tabular = evaluate_blackjack_agent(agent_tabular, env, 1000)\n",
    "print(f\"Win rate of the agent: {win_rate_tabular:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the episode rewards, episode length and training error in one figure\n",
    "fig, axs = plt.subplots(1, 3, figsize=(10, 4))\n",
    "\n",
    "# np.convolve will compute the rolling mean for 100 episodes\n",
    "axs[0].plot(np.convolve(env.return_queue, np.ones(100)))\n",
    "axs[0].set_title(\"Episode Rewards\")\n",
    "axs[0].set_xlabel(\"Episode\")\n",
    "axs[0].set_ylabel(\"Reward\")\n",
    "\n",
    "axs[1].plot(np.convolve(env.length_queue, np.ones(100)))\n",
    "axs[1].set_title(\"Episode Lengths\")\n",
    "axs[1].set_xlabel(\"Episode\")\n",
    "axs[1].set_ylabel(\"Length\")\n",
    "\n",
    "axs[2].plot(np.convolve(agent_tabular.training_error, np.ones(100)))\n",
    "axs[2].set_title(\"Training Error\")\n",
    "axs[2].set_xlabel(\"Episode\")\n",
    "axs[2].set_ylabel(\"Temporal Difference\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "q_values = unpack_qvalues_dict(agent_tabular.q_values)\n",
    "\n",
    "# plot the Q-values for the state-action pairs\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "for i, usable_ace in enumerate([False, True]):\n",
    "    im = axs[i].imshow(q_values[:, :, int(usable_ace)], aspect=\"auto\")\n",
    "    axs[i].set_title(f\"Chosen move with usable ace = {usable_ace}\")\n",
    "    axs[i].set_xlabel(\"Dealer's face-up card\")\n",
    "    axs[i].set_ylabel(\"Player's sum\")\n",
    "    fig.colorbar(im, ax=axs[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Defining and Training a Deep Q-Network agent\n",
    "\n",
    "In Task 2, we will implement an agent that will use **TD-learning** and an **Artificial Neural Network** to learn how to play Blackjack.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "<b>üí° Tips</b> \n",
    "\n",
    "- Review the following elements of theory, you will need them later.\n",
    "\n",
    "</div>\n",
    "\n",
    "### What is a Q-Network?\n",
    "\n",
    "A **Q-Network** is a **neural network** that approximates the **Q-value function** $ Q(s, a) $ in **Deep Q-Learning (DQN)**. Instead of using a table to store Q-values for each state-action pair, a Q-Network generalizes Q-values across **large or continuous state spaces**.\n",
    "\n",
    "**Definition:**\n",
    "\n",
    "$$\n",
    "Q(s, a; \\theta) \\approx Q^*(s, a)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ \\theta $ are the **neural network weights**.\n",
    "- $ Q^*(s, a) $ is the **optimal Q-value function**.\n",
    "\n",
    "---\n",
    "\n",
    "### How Q-Networks Differ from Tabular Q-Learning\n",
    "\n",
    "| Feature  | **Tabular Q-Learning** | **Q-Network (DQN)** |\n",
    "|----------|---------------------|------------------|\n",
    "| **Representation** | Uses a **Q-table**: $ Q(s, a) $ stored explicitly | Uses a **neural network** to approximate $ Q(s, a) $ |\n",
    "| **State Space** | Works well for **small** discrete state spaces | Scales to **large** or continuous state spaces |\n",
    "| **Generalization** | No generalization, each state-action pair is stored explicitly | Generalizes Q-values for unseen states |\n",
    "| **Memory Usage** | Requires large memory for big state spaces | More memory-efficient due to learned representations |\n",
    "| **Training Updates** | Q-values updated directly in a lookup table | Neural network weights updated via **gradient descent** |\n",
    "| **Exploration** | Uses **epsilon-greedy** to explore | Same, but exploration can be improved with **policy-based methods** |\n",
    "| **Performance in Complex Environments** | Limited to small environments | Works well in high-dimensional spaces like **Atari games** |\n",
    "\n",
    "---\n",
    "\n",
    "### How Q-Networks Work (DQN Algorithm)\n",
    "\n",
    "1. **Initialize** a neural network $ Q(s, a; \\theta) $ with random weights.\n",
    "2. **At each step:**\n",
    "   - Select an action using **epsilon-greedy** exploration.\n",
    "   - Execute the action, observe the reward and next state.\n",
    "   - Compute the **TD target** using the Bellman equation:\n",
    "\n",
    "     $$\n",
    "     y = r + \\gamma \\max_{a'} Q(s', a'; \\theta)\n",
    "     $$\n",
    "\n",
    "   - Compute the **loss function**:\n",
    "\n",
    "     $$\n",
    "     L(\\theta) = (y - Q(s, a; \\theta))^2\n",
    "     $$\n",
    "\n",
    "   - **Update network weights** using **gradient descent**.\n",
    "3. **Repeat** until convergence.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Implement a simple Q-Network agent\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Implement the class `QNetworkModel` in the cell bellow with the following architecture\n",
    "    - layer 1 : Dense layer with 64 neurons and relu activation function\n",
    "    - layer 2 : Dense layer with 64 neurons and relu activation function\n",
    "    - layer 3 : Dense layer with as many neurons as possible actions.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetworkModel(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Implements a simple feedforward neural network with two hidden layers for Q-learning.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, n_actions: int):\n",
    "        super(QNetworkModel, self).__init__()\n",
    "        # TODO: your code here\n",
    "        self.dense1 = ...\n",
    "        self.dense2 = ...\n",
    "        self.dense3 = ...\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Method to call the model on the input tensors. \n",
    "        It is called automatically when the model is called with .predict() or .fit().\n",
    "        \"\"\"\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        return self.dense3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Implement the function `get_action` in the class `QNetworkAgent` with the eplison greedy policy.\n",
    "- Implement the function `update` in the class `QNetworkAgent` that uses TD Learning to update the `QNetworkAgent.DQN` weights using `tf.keras.Model.fit()`.\n",
    "- Implement the function `decay_epsilon` in the class `QNetworkAgent` that updates the agent's epsilon parameters until it reaches its final value.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetworkAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        learning_rate: float,\n",
    "        initial_epsilon: float,\n",
    "        epsilon_decay: float,\n",
    "        final_epsilon: float,\n",
    "        discount_factor: float = 0.90,\n",
    "    ):\n",
    "        \"\"\"Initialize a Reinforcement Learning agent with an empty dictionary\n",
    "        of state-action values (q_values), a learning rate and an epsilon.\n",
    "\n",
    "        Args:\n",
    "            env: The training environment\n",
    "            learning_rate: The learning rate\n",
    "            initial_epsilon: The initial epsilon value\n",
    "            epsilon_decay: The decay increment for the epsilon value\n",
    "            final_epsilon: The final epsilon value \n",
    "            discount_factor: The discount factor for computing the Q-value\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.DQN = QNetworkModel(n_actions=2) # Q-values for each state-action pair, it is of the same shape as the action space\n",
    "        self.DQN.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mse')\n",
    "\n",
    "        self.lr = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        self.epsilon = initial_epsilon # epsilon value of the agent\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "\n",
    "        self.training_error = [] # temporal difference error for each update\n",
    "\n",
    "    def get_action(self, obs: tuple[int, int, bool]) -> int:\n",
    "        \"\"\"\n",
    "        Returns the action to take given the observation using an epsilon-greedy policy.\n",
    "\n",
    "        Args:\n",
    "            obs(tuple): The observation of the environment\n",
    "                obs[0](int): The player's current sum\n",
    "                obs[1](int): The dealer's face-up card\n",
    "                obs[2](bool): Whether the player has a usable ace\n",
    "\n",
    "        Returns:\n",
    "            The action to take (0 for stick, 1 for hit)\n",
    "        \"\"\"\n",
    "        # TODO: your code here\n",
    "        # with probability epsilon return a random action to explore the environment\n",
    "        ...\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        obs: tuple[int, int, bool],\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        terminated: bool,\n",
    "        next_obs: tuple[int, int, bool],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Updates the Q-values of one or multiple state-action pairs using TD-learning.\n",
    "\n",
    "        Args:\n",
    "            obs (tuple or np.ndarray): The observation(s) of the environment.\n",
    "                If a single observation is provided, it is a tuple of the form:\n",
    "                    obs[0] (int): The player's current sum.\n",
    "                    obs[1] (int): The dealer's face-up card.\n",
    "                    obs[2] (bool): Whether the player has a usable ace.\n",
    "                If a batch of observations is provided, it is a NumPy array of shape (batch_size, 3).\n",
    "            \n",
    "            action (int or np.ndarray): The action(s) taken.\n",
    "                If a single action is provided, it is an integer (0 for stick, 1 for hit).\n",
    "                If a batch of actions is provided, it is a NumPy array of shape (batch_size,).\n",
    "            \n",
    "            reward (float or np.ndarray): The reward(s) received.\n",
    "                If a single reward is provided, it is a float.\n",
    "                If a batch of rewards is provided, it is a NumPy array of shape (batch_size,).\n",
    "\n",
    "            terminated (bool or np.ndarray): Whether the episode has terminated.\n",
    "                If a single termination flag is provided, it is a boolean.\n",
    "                If a batch of termination flags is provided, it is a NumPy array of shape (batch_size,).\n",
    "\n",
    "            next_obs (tuple or np.ndarray): The next observation(s) of the environment.\n",
    "                If a single observation is provided, it is a tuple of the form:\n",
    "                    next_obs[0] (int): The player's current sum.\n",
    "                    next_obs[1] (int): The dealer's face-up card.\n",
    "                    next_obs[2] (bool): Whether the player has a usable ace.\n",
    "                If a batch of next observations is provided, it is a NumPy array of shape (batch_size, 3).\n",
    "\n",
    "        Updated parameters:\n",
    "            self.DQN: The Q-network, which is trained using a batch of experience samples.\n",
    "            self.training_error: The temporal difference error for each update, tracked over the batch.\n",
    "        \"\"\"\n",
    "\n",
    "        # Ensure obs and next_obs are arrays of shape (-1, 3)\n",
    "        if isinstance(obs, tuple):\n",
    "            obs = np.array([obs])\n",
    "        if isinstance(next_obs, tuple):\n",
    "            next_obs = np.array([next_obs])\n",
    "\n",
    "        # Calculate the future Q-value. If the episode has terminated, the future Q-value is 0, otherwise it is the maximum Q-value of the next observation\n",
    "        # TODO: your code here\n",
    "        future_q_value = ...\n",
    "\n",
    "        # Calculate the TD target. \n",
    "        # The TD target is what the DQN's output will be evaluated against. \n",
    "        # For the action taken, the TD target is the reward plus the discounted future Q-value.\n",
    "        # For the other actions, the TD target is the current Q-value.\n",
    "        # TODO: your code here\n",
    "        TD_target = self.DQN.predict(obs, verbose=0)\n",
    "        TD_target[np.arange(obs.shape[0]), action] = ...\n",
    "\n",
    "        # Update the DQN model using the observations and the TD target with model.fit\n",
    "        # TODO: your code here\n",
    "        ...\n",
    "\n",
    "        # Track training error, which is the mean temporal difference error for the action taken over the batch\n",
    "        # TODO: your code here\n",
    "        ...\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"\n",
    "        Decays the epsilon value.\n",
    "        \n",
    "        The epsilon value is decayed by the decay increment\n",
    "        and is clipped to the final epsilon value.\n",
    "\n",
    "        Updated parameters:\n",
    "            self.epsilon: The epsilon value of the agent\n",
    "        \"\"\"\n",
    "        # TODO: your code here\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Train the agent\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Implement the function `train_blackjack_DQN_agent` bellow. Each comment `# TODO` indicates that some code must be implemented.\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "<b>üí° Tips</b> \n",
    "\n",
    "- Run `help(gym.envs.toy_text.blackjack.BlackjackEnv.step)` to print the method's doctring.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_blackjack_DQN_agent(agent, env, n_episodes:int)->None:\n",
    "    \"\"\"\n",
    "    Trains the agent using the Q-learning algorithm.\n",
    "\n",
    "    Args:\n",
    "        agent: The Q-learning agent\n",
    "        env: The training environment\n",
    "        n_episodes(int): The number of episodes to train the agent\n",
    "\n",
    "    \"\"\"\n",
    "    for episode in tqdm(range(n_episodes)):\n",
    "        obs, info = env.reset() # reset the environment and get the initial observation\n",
    "        done = False\n",
    "\n",
    "        # play one episode\n",
    "        while not done:\n",
    "            # get the action from the agent\n",
    "            # TODO: your code here\n",
    "            action = ...\n",
    "\n",
    "            # take a step in the environment using the action with BlackjackEnv.step and get the next observation, reward, termination, and truncation\n",
    "            # TODO: your code here\n",
    "            next_obs, reward, terminated, truncated, info = ...\n",
    "\n",
    "            # update the agent with the observation, action, reward, termination, and next observation\n",
    "            # TODO: your code here\n",
    "            ...\n",
    "\n",
    "            # update if the environment is done (terminated or truncated is True)\n",
    "            # TODO: your code here\n",
    "            done = ...\n",
    "\n",
    "            # update the observation to the next observation\n",
    "            # TODO: your code here\n",
    "            obs = ...\n",
    "\n",
    "        # update the agent's epsilon value\n",
    "        # TODO: your code here\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 2000\n",
    "env = gym.make('Blackjack-v1', natural=False, sab=False)\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=n_episodes)\n",
    "\n",
    "# Set hyperparameters\n",
    "learning_rate = 0.01\n",
    "start_epsilon = 1.0\n",
    "epsilon_decay = start_epsilon / (n_episodes)  # reduce the exploration over time\n",
    "final_epsilon = 0.1\n",
    "\n",
    "\n",
    "# Create the agent\n",
    "agent = QNetworkAgent(\n",
    "    env=env,\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=start_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    final_epsilon=final_epsilon,\n",
    ")\n",
    "\n",
    "# Train the agent\n",
    "train_blackjack_DQN_agent(agent, env, n_episodes)\n",
    "\n",
    "# Print the win rate of the agent\n",
    "win_rate_dqn = evaluate_blackjack_agent(agent, env, 1000)\n",
    "print(f\"Win rate of the agent: {win_rate_dqn:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Run the cells bellow to train the agent and visualise the results.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the episode rewards, episode length and training error in one figure\n",
    "fig, axs = plt.subplots(1, 3, figsize=(10, 4))\n",
    "\n",
    "# np.convolve will compute the rolling mean for 100 episodes\n",
    "axs[0].plot(np.convolve(env.return_queue, np.ones(10)))\n",
    "axs[0].set_title(\"Episode Rewards\")\n",
    "axs[0].set_xlabel(\"Episode\")\n",
    "axs[0].set_ylabel(\"Reward\")\n",
    "\n",
    "axs[1].plot(np.convolve(env.length_queue, np.ones(10)))\n",
    "axs[1].set_title(\"Episode Lengths\")\n",
    "axs[1].set_xlabel(\"Episode\")\n",
    "axs[1].set_ylabel(\"Length\")\n",
    "\n",
    "axs[2].plot(np.convolve(agent.training_error, np.ones(10)))\n",
    "axs[2].set_title(\"Training Error\")\n",
    "axs[2].set_xlabel(\"Episode\")\n",
    "axs[2].set_ylabel(\"Temporal Difference\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "q_values = get_best_actions_from_DQN(agent)\n",
    "\n",
    "# plot the Q-values for the state-action pairs\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "for i, usable_ace in enumerate([False, True]):\n",
    "    im = axs[i].imshow(q_values[:, :, int(usable_ace)], aspect=\"auto\")\n",
    "    axs[i].set_title(f\"Chosen move with usable ace = {usable_ace}\")\n",
    "    axs[i].set_xlabel(\"Dealer's face-up card\")\n",
    "    axs[i].set_ylabel(\"Player's sum\")\n",
    "    fig.colorbar(im, ax=axs[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Improving the DQN agent's performences\n",
    "\n",
    "## 3.1 Speeding up learning with experience replay\n",
    "\n",
    "\n",
    "During our experiments, we observed that the **performance of the Deep Q-Network (DQN) was poor**, and training was **extremely slow**. This can be explained by:\n",
    "\n",
    "1. **Lack of Batch Updates**: The agent updated the Q-network **after every step**, using a single experience sample. This is inneficient as it **does not take advantage of vectorization**.\n",
    "2. **Correlated Training Data**: Each update was based on the **most recent transition**, meaning that consecutive updates were highly correlated.  \n",
    "3. **Inefficient Use of Data**: Once a transition was used for training, it was discarded, leading to **slow convergence**.  \n",
    "\n",
    "To address these issues, we introduce **Experience Replay**, a technique that significantly improves training efficiency and stability.\n",
    "\n",
    "---\n",
    "\n",
    "### Experience Replay\n",
    "\n",
    "**Experience Replay** is a mechanism where the agent **stores past experiences** (state, action, reward, next state, done flag) in a **replay buffer**. Instead of updating the Q-network after every step, the agent **samples a batch of past experiences** and trains on them in a more efficient way.\n",
    "\n",
    "\n",
    "**Definition:**\n",
    "\n",
    "1. **Store Experiences**: Every transition $(s, a, r, s', \\text{done})$ is stored in a **replay buffer** (a fixed-size memory).  \n",
    "2. **Sample a Batch**: Instead of using only the latest experience, we **randomly sample a batch** of experiences from the buffer.  \n",
    "3. **Compute TD Target**: For each sampled transition, compute the **TD target**:\n",
    "\n",
    "   $$\n",
    "   y = r + \\gamma \\max_{a'} Q(s', a')\n",
    "   $$\n",
    "\n",
    "4. **Train on the Batch**: The Q-network is updated using a **mini-batch gradient descent** step, rather than a single experience.  \n",
    "5. **Repeat**: This process continues throughout training.\n",
    "\n",
    "\n",
    "**Implementation:**\n",
    "\n",
    "Training a DQN with experience Replay consists of two distinct **phases**:\n",
    "\n",
    "**Initialization Phase** (Filling the Replay Buffer)  \n",
    "- Initially, the agent **collects experience** by interacting with the environment **without training**.  \n",
    "- The DQN **runs as before**, updating after each step, but only **stores transitions** in the buffer.  \n",
    "- This ensures that we have **diverse experiences** before starting training.\n",
    "\n",
    "**Batch Training Phase** (Experience Replay Training)  \n",
    "- Once the **buffer is filled**, we start **training using mini-batches** from stored experiences.  \n",
    "- Instead of learning from just the latest experience, we **sample a batch of past transitions** randomly.\n",
    "- The agent **trains more efficiently** by learning from **uncorrelated experiences**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Using the `ExperienceReplayBuffer`class defined in `utils.py`, implement the function `train_blackjack_DQN_agent_experience_replay` defined in the cell bellow.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_blackjack_DQN_agent_experience_replay(agent, env:gym.envs.toy_text.blackjack.BlackjackEnv, n_episodes:int, batch_size:int, buffer_size:int)->None:\n",
    "    \"\"\"\n",
    "    Trains the agent using the Q-learning algorithm.\n",
    "\n",
    "    Args:\n",
    "        agent: The Q-learning agent\n",
    "        env: The training environment\n",
    "        n_episodes(int): The number of episodes to train the agent\n",
    "        batch_size(int): The batch size for experience replay\n",
    "        buffer_size(int): The size of the experience replay buffer\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # Define the experience replay buffer\n",
    "    # TODO: your code here\n",
    "    experience = ...\n",
    "    \n",
    "\n",
    "    for episode in tqdm(range(n_episodes)):\n",
    "        obs, info = env.reset() # reset the environment and get the initial observation\n",
    "        done = False\n",
    "\n",
    "        # play one episode\n",
    "        while not done:\n",
    "            # get the action from the agent\n",
    "            # TODO: your code here\n",
    "            action = ...\n",
    "\n",
    "            # take a step in the environment using the action with BlackjackEnv.step and get the next observation, reward, termination, and truncation\n",
    "            # TODO: your code here\n",
    "            next_obs, reward, terminated, truncated, info = ...\n",
    "\n",
    "            # add the experience to the buffer\n",
    "            # TODO: your code here\n",
    "            ...\n",
    "\n",
    "            # update the agent using experience replay if the buffer has been filled, otherwise update the agent using the last experience\n",
    "            if experience.full: # if the buffer is full, sample a batch of experiences with ExperienceReplayBuffer.sample() and update the agent\n",
    "                # TODO: your code here\n",
    "                ...\n",
    "            else: # we're in the initial phase of training, update the agent with the last experience\n",
    "                # TODO: your code here\n",
    "                ...\n",
    "            \n",
    "            # update if the environment is done (terminated or truncated is True)\n",
    "            # TODO: your code here\n",
    "            done = ...\n",
    "            # update the observation to the next observation\n",
    "            # TODO: your code here\n",
    "            obs = ...\n",
    "\n",
    "        # update the agent's epsilon value\n",
    "        # TODO: your code here\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Run the cells bellow to train the agent and visualise the results.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 2000\n",
    "env = gym.make('Blackjack-v1', natural=False, sab=False)\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=n_episodes)\n",
    "\n",
    "# Set hyperparameters\n",
    "learning_rate = 0.01\n",
    "start_epsilon = 1.0\n",
    "epsilon_decay = start_epsilon / (n_episodes)  # reduce the exploration over time\n",
    "final_epsilon = 0.1\n",
    "\n",
    "\n",
    "# Create the agent\n",
    "agent = QNetworkAgent(\n",
    "    env=env,\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=start_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    final_epsilon=final_epsilon,\n",
    ")\n",
    "\n",
    "# Train the agent\n",
    "train_blackjack_DQN_agent_experience_replay(agent, env, n_episodes, 10, 1000)\n",
    "\n",
    "# Print the win rate of the agent\n",
    "win_rate_exp_replay_dqn = evaluate_blackjack_agent(agent, env, 1000)\n",
    "print(f\"Win rate of the agent: {win_rate_exp_replay_dqn:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the episode rewards, episode length and training error in one figure\n",
    "fig, axs = plt.subplots(1, 3, figsize=(10, 4))\n",
    "\n",
    "# np.convolve will compute the rolling mean for 5 episodes\n",
    "axs[0].plot(np.convolve(env.return_queue, np.ones(10))[10:-10])\n",
    "axs[0].set_title(\"Episode Rewards\")\n",
    "axs[0].set_xlabel(\"Episode\")\n",
    "axs[0].set_ylabel(\"Reward\")\n",
    "\n",
    "axs[1].plot(np.convolve(env.length_queue, np.ones(10))[10:-10])\n",
    "axs[1].set_title(\"Episode Lengths\")\n",
    "axs[1].set_xlabel(\"Episode\")\n",
    "axs[1].set_ylabel(\"Length\")\n",
    "\n",
    "axs[2].plot(np.convolve(agent.training_error, np.ones(10))[10:-10])\n",
    "axs[2].set_title(\"Training Error\")\n",
    "axs[2].set_xlabel(\"Episode\")\n",
    "axs[2].set_ylabel(\"Temporal Difference\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "q_values = get_best_actions_from_DQN(agent)\n",
    "\n",
    "# plot the Q-values for the state-action pairs\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "for i, usable_ace in enumerate([False, True]):\n",
    "    im = axs[i].imshow(q_values[:, :, int(usable_ace)], aspect=\"auto\")\n",
    "    axs[i].set_title(f\"Chosen move with usable ace = {usable_ace}\")\n",
    "    axs[i].set_xlabel(\"Dealer's face-up card\")\n",
    "    axs[i].set_ylabel(\"Player's sum\")\n",
    "    fig.colorbar(im, ax=axs[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Increasing stability with double DQN\n",
    "\n",
    "Even after implementing **Experience Replay**, we observed that **DQN training remains highly unstable**.  \n",
    "From one training run to another, the **final performance varies significantly**, and the agent sometimes learns **suboptimal policies**. This is because:\n",
    "\n",
    "1. **Overestimation Bias in Q-Values**  \n",
    "   - In standard DQN, we use the **max Q-value** from the target network to compute the TD target:\n",
    "     $$\n",
    "     y = r + \\gamma \\max_{a'} Q(s', a')\n",
    "     $$\n",
    "   - This causes the network to **overestimate Q-values**, since it selects the same action for evaluation and target calculation.\n",
    "   - These overestimations **compound over time**, leading to **unstable training**.\n",
    "\n",
    "2. **Large Training Variability Across Runs**  \n",
    "   - Due to overestimation, **small differences in early learning** lead to **major differences in final performance**.\n",
    "   - One run might converge to an **optimal policy**, while another might **fail completely**.\n",
    "\n",
    "---\n",
    "\n",
    "### Double DQN (DDQN)\n",
    "\n",
    "**Double DQN (DDQN)** is a simple yet powerful modification to DQN that **reduces overestimation bias** and makes training more stable.  \n",
    "\n",
    "\n",
    "Instead of using the **same Q-network** for both action selection and value estimation, **Double DQN separates these roles** by using:  \n",
    "- The **main network** to **select the action** ($a^*$).\n",
    "- The **target network** to **evaluate its Q-value**.\n",
    "\n",
    "**Definition:**  \n",
    "\n",
    "Instead of computing the target as:  \n",
    "$$\n",
    "y = r + \\gamma \\max_{a'} Q_{\\text{target}}(s', a')\n",
    "$$\n",
    "We modify it to:  \n",
    "$$\n",
    "y = r + \\gamma Q_{\\text{target}}(s', \\arg\\max_{a'} Q_{\\text{main}}(s', a'))\n",
    "$$\n",
    "Here‚Äôs what changes:  \n",
    "1. **Action Selection:** The **main network** picks the best action $ a^* = \\arg\\max Q_{\\text{main}}(s', a') $.  \n",
    "2. **Value Estimation:** The **target network** evaluates this action‚Äôs value.  \n",
    "\n",
    "This prevents the **overestimation problem** and makes training more **stable** across different runs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Implement the function `get_action` in the class `DoubleQNetworkAgent` with the eplison greedy policy with the main DQN.\n",
    "- Implement the function `update` in the class `DoubleQNetworkAgent` that uses TD Learning to update the `DoubleQNetworkAgent.mainDQN` weights using doubleDQN `tf.keras.Model.fit()`.\n",
    "- Implement the function `update_target_DQN` in the class `DoubleQNetworkAgent` that updates the target DQN with the main DQN's weights using [`DoubleQNetworkAgent.mainDQN.get_weights()`](https://www.tensorflow.org/api_docs/python/tf/keras/Layer#get_weights) and [``DoubleQNetworkAgent.targetDQN.set_weights`](https://www.tensorflow.org/api_docs/python/tf/keras/Layer#set_weights).\n",
    "- Implement the function `decay_epsilon` in the class `DoubleQNetworkAgent` that updates the agent's epsilon parameters until it reaches its final value.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleQNetworkAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        learning_rate: float,\n",
    "        initial_epsilon: float,\n",
    "        epsilon_decay: float,\n",
    "        final_epsilon: float,\n",
    "        discount_factor: float = 0.90,\n",
    "    ):\n",
    "        \"\"\"Initialize a Reinforcement Learning agent with an empty dictionary\n",
    "        of state-action values (q_values), a learning rate and an epsilon.\n",
    "\n",
    "        Args:\n",
    "            env: The training environment\n",
    "            learning_rate: The learning rate\n",
    "            initial_epsilon: The initial epsilon value\n",
    "            epsilon_decay: The decay increment for the epsilon value\n",
    "            final_epsilon: The final epsilon value \n",
    "            discount_factor: The discount factor for computing the Q-value\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "\n",
    "        self.mainDQN = QNetworkModel(n_actions=2) # Q-values for each state-action pair, it is of the same shape as the action space\n",
    "        self.mainDQN.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mse')\n",
    "\n",
    "        self.targetDQN = QNetworkModel(n_actions=2) # Q-values for each state-action pair, it is of the same shape as the action space\n",
    "        self.targetDQN.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mse')\n",
    "\n",
    "        self.lr = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        self.epsilon = initial_epsilon # epsilon value of the agent\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "\n",
    "        self.training_error = [] # temporal difference error for each update\n",
    "\n",
    "    def get_action(self, obs: tuple[int, int, bool]) -> int:\n",
    "        \"\"\"\n",
    "        Returns the action to take given the observation using an epsilon-greedy policy.\n",
    "\n",
    "        Args:\n",
    "            obs(tuple): The observation of the environment\n",
    "                obs[0](int): The player's current sum\n",
    "                obs[1](int): The dealer's face-up card\n",
    "                obs[2](bool): Whether the player has a usable ace\n",
    "\n",
    "        Returns:\n",
    "            The action to take (0 for stick, 1 for hit)\n",
    "        \"\"\"\n",
    "        # TODO: your code here\n",
    "        ...\n",
    "\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        obs: tuple[int, int, bool],\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        terminated: bool,\n",
    "        next_obs: tuple[int, int, bool],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Updates the Q-value of the state-action pair using Double DQN.\n",
    "        \"\"\"\n",
    "        # Convert input to NumPy arrays\n",
    "        if isinstance(obs, tuple):\n",
    "            obs = np.array([obs])\n",
    "        if isinstance(next_obs, tuple):\n",
    "            next_obs = np.array([next_obs])\n",
    "\n",
    "        # Get action selection from main network for next observations \n",
    "        # hint: for each next observation in the batch, get the action {0 or 1} with the highest Q-value outputed by the main network\n",
    "        # TODO: your code here\n",
    "        best_action = ...\n",
    "\n",
    "        # Use target network to predict Q-values for the best action for the next observation\n",
    "        # TODO: your code here\n",
    "        target_q_values = ...\n",
    "\n",
    "        # Compute future Q-values over the batch\n",
    "        # hint: if an episode has terminated, the future Q-value is 0,\n",
    "        # otherwise the future Q-values are the target Q-value associated with the best action in the next observation as predicted by the target network\n",
    "        # TODO: your code here\n",
    "        future_q_value = ...\n",
    "\n",
    "        # Compute TD target\n",
    "        # We start with the current Q-values, and we update the Q-value of the action taken\n",
    "        TD_target = self.mainDQN.predict(obs, verbose=0)  # Start with current Q-values\n",
    "        TD_target[np.arange(obs.shape[0]), action] = ... # TODO: your code here\n",
    "\n",
    "        # Update the DQN model using the observations and the TD target with model.fit\n",
    "        # TODO: your code here\n",
    "        ...\n",
    "\n",
    "        # Track training error, which is the mean temporal difference error for the action taken over the batch\n",
    "        # TODO: your code here\n",
    "        ...\n",
    "\n",
    "\n",
    "    def update_target_DQN(self):\n",
    "        \"\"\"\n",
    "        Update the target DQN with the weights of the main DQN.\n",
    "\n",
    "        Updated parameters:\n",
    "            self.targetDQN: The target DQN model\n",
    "        \"\"\"\n",
    "        # Get the weights of the main DQN, and set them as the weights of the target DQN\n",
    "        # TODO: your code here\n",
    "        ...\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"\n",
    "        Decays the epsilon value.\n",
    "        \n",
    "        The epsilon value is decayed by the decay increment\n",
    "        and is clipped to the final epsilon value.\n",
    "\n",
    "        Updated parameters:\n",
    "            self.epsilon: The epsilon value of the agent\n",
    "        \"\"\"\n",
    "        # TODO: your code here\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Implement the function `train_blackjack_double_DQN_agent_experience_replay` defined in the cell bellow. It's the exact same function as `train_blackjack_DQN_agent_experience_replay`, but wit the addition of having to update the weights of the target network periodically.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_blackjack_double_DQN_agent_experience_replay(agent, env:gym.envs.toy_text.blackjack.BlackjackEnv, n_episodes:int, batch_size:int, buffer_size:int, n_episodes_for_target_DQN_update=5)->None:\n",
    "    \"\"\"\n",
    "    Trains the agent using the Q-learning algorithm. The agent uses Double DQN and experience replay.\n",
    "    The weights of the target DQN are updated every n_episodes_for_target_DQN_update episodes.\n",
    "\n",
    "    Args:\n",
    "        agent: The Q-learning agent\n",
    "        env: The training environment\n",
    "        n_episodes(int): The number of episodes to train the agent\n",
    "        batch_size(int): The batch size for experience replay\n",
    "        buffer_size(int): The size of the experience replay buffer\n",
    "        n_episodes_for_target_DQN_update(int): The number of episodes before updating the target DQN\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # TODO: your code here (this time you don't get help from me)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Run the cells bellow to train the agent and visualise the results.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 2000\n",
    "# Set hyperparameters\n",
    "learning_rate = 0.01\n",
    "start_epsilon = 1.0\n",
    "epsilon_decay = start_epsilon / (n_episodes)  # reduce the exploration over time\n",
    "final_epsilon = 0.1\n",
    "\n",
    "\n",
    "# Create the agent\n",
    "agent_double_DQN = DoubleQNetworkAgent(\n",
    "    env=env,\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=start_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    final_epsilon=final_epsilon,\n",
    ")\n",
    "\n",
    "# Train the agent\n",
    "train_blackjack_double_DQN_agent_experience_replay(agent_double_DQN, env, n_episodes, 10, 5)\n",
    "\n",
    "# Print the win rate of the agent\n",
    "win_rate_double_dqn = evaluate_blackjack_agent(agent_double_DQN, env, 1000)\n",
    "print(f\"Win rate of the agent: {win_rate_double_dqn:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the episode rewards, episode length and training error in one figure\n",
    "fig, axs = plt.subplots(1, 3, figsize=(10, 4))\n",
    "\n",
    "# np.convolve will compute the rolling mean for 5 episodes\n",
    "axs[0].plot(np.convolve(env.return_queue, np.ones(10))[10:-10])\n",
    "axs[0].set_title(\"Episode Rewards\")\n",
    "axs[0].set_xlabel(\"Episode\")\n",
    "axs[0].set_ylabel(\"Reward\")\n",
    "\n",
    "axs[1].plot(np.convolve(env.length_queue, np.ones(10))[10:-10])\n",
    "axs[1].set_title(\"Episode Lengths\")\n",
    "axs[1].set_xlabel(\"Episode\")\n",
    "axs[1].set_ylabel(\"Length\")\n",
    "\n",
    "axs[2].plot(np.convolve(agent_double_DQN.training_error, np.ones(10))[10:-10])\n",
    "axs[2].set_title(\"Training Error\")\n",
    "axs[2].set_xlabel(\"Episode\")\n",
    "axs[2].set_ylabel(\"Temporal Difference\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "q_values = get_best_actions_from_DQN(agent_double_DQN, double_DQN = True)\n",
    "\n",
    "# plot the Q-values for the state-action pairs\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "for i, usable_ace in enumerate([False, True]):\n",
    "    im = axs[i].imshow(q_values[:, :, int(usable_ace)], aspect=\"auto\")\n",
    "    axs[i].set_title(f\"Chosen move with usable ace = {usable_ace}\")\n",
    "    axs[i].set_xlabel(\"Dealer's face-up card\")\n",
    "    axs[i].set_ylabel(\"Player's sum\")\n",
    "    fig.colorbar(im, ax=axs[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Run the cells bellow to compare the performances of all the agents we trained in this lab.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print benchmark results\n",
    "print(\"Benchmark results:\")\n",
    "print(f\"Win rate of the random policy: {win_rate_random:.2f}\")\n",
    "print(f\"Win rate of the basic strategy: {win_rate_basic:.2f}\")\n",
    "\n",
    "# Print all agents' win rates\n",
    "print(\"\\nWin rates of the agents:\")\n",
    "print(f\"Win rate of the tabular agent: {win_rate_tabular:.2f}\")\n",
    "print(f\"Win rate of the DQN agent: {win_rate_dqn:.2f}\")\n",
    "print(f\"Win rate of the DQN agent with experience replay: {win_rate_exp_replay_dqn:.2f}\")\n",
    "print(f\"Win rate of the Double DQN agent with experience replay: {win_rate_double_dqn:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 To go further\n",
    "\n",
    "In this lab, we have attempted to use RL techniques to learn policies to maximise the changes of winning a simple blackjack game. You are now equiped with a basic understanding of these technics, but if you want to truelly master them, you need to dig deeper into the field. \n",
    "\n",
    "The following is a list of ressources you can use should you wish to learn more about reinforcment learning.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<h4>üë©‚Äçüíª [Optional]</h4>\n",
    "\n",
    "- Train agents on [more complex environments](https://gymnasium.farama.org/environments/third_party_environments/) with Gymnasium.\n",
    "- Go through Hugging Face's free online course on [Deep Reinforcement Learning](https://huggingface.co/learn/deep-rl-course/en/unit0/introduction).\n",
    "- Read academic papers on RL, such as [`SIMPLIFYING DEEP TEMPORAL DIFFERENCE LEARNING`](https://arxiv.org/pdf/2407.04811v2).\n",
    "- Get into [Transformers Reinforcement Learning](https://huggingface.co/docs/trl/en/index). \n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amls2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
